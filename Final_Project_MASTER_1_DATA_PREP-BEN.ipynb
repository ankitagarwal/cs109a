{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "**<font size=5><center>Predicting Default Rates for Lending Club</center></font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>Devon Luongo</font> <br>\n",
    "<font size=4>Harvard University</font> <br>\n",
    "<font size=3>deluongo@gmail.com</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>Ben Yuen</font> <br>\n",
    "<font size=4>Harvard University</font> <br>\n",
    "<font size=3>benyuen@hotmail.com</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>Bryn Clarke</font> <br>\n",
    "<font size=4>Harvard University</font> <br>\n",
    "<font size=3></font>clarke.bryn@gmail.com<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>Ankit Agarwal</font> <br>\n",
    "<font size=4>Harvard University</font> <br>\n",
    "<font size=3>ankit.frnz@gmail.com</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font size=4><center>Abstract</center></font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>**!!(Insert Abstract)!!**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lending Club is an online marketplace that fascilitates the lending money to individuals or businesses through online services that match lenders directly with borrowers. This practice is called Peer-to-Peer (P2P) Lending. With an A+ BBB rating, Lending Club offers an attractive alternative to bonds for steady investment income. Lending Club also states that borrowers are able to reduce their loans by an average of 30%.\n",
    "\n",
    "\"Since peer-to-peer lending companies offering these services operate entirely online, they can run with lower overhead and provide the service more cheaply than traditional financial institutions. As a result, lenders often earn higher returns compared to savings and investment products offered by banks, while borrowers can borrow money at lower interest rates, even after the P2P lending company has taken a fee for providing the match-making platform and credit checking the borrower.\"\n",
    "\n",
    "The downside to Lending Club however, is that most peer-to-peer loans are unsecured personal loans, meaning lenders incur a higher risk than comparable steady income investments like Treasury Bonds. Therefore, the investment quality of each loan depends largly on three factors: the expected return, the level of risk aversion, and the risk incurred. The first factor is built into the contract of the loan, and the second factor is personal preference. Therefore, the primary variable that that investors must identify when compoaring P2P Lending with other investment opportunities is the default risk for each loan. \n",
    "\n",
    "For this reason, Lending Club offers lenders a dataset containing a comprehensive list of features that can be employed to make better lending decisions. Detailed information for every loan have been issued by Lending Club from 2007 to 2015. Among**!!(Insert Num Predictors)!!** features or predictors in total, the dataset includes a borrowerâ€™s annual incomes, zip codes, revolving balances, and purpose for borrowing.\n",
    "\n",
    "Our objective was to build on similar projects conducted in recent years to construct a model for default risk to help lenders decrease the risk they occur when lending. We trained and tested a range of models in an attempt to identify the best performing model. Additionally, we constructed a cost based model aimed at maximizing Lending Clubs overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Related Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!!(Rewrite After Models Complete)!!** \n",
    "Prior projects like **!!(\"Predicting borrowers chance of defaulting on credit loans\" [1])!!** have set great examples of applying machine learning to improve loan default prediction in a Kaggle competition, and authors for**!!(\"Predicting Probability of Loan Default\" [2])!!**  have shown that Random Forest appeared to be the best performing model on the Kaggle data. \n",
    "\n",
    "However, despite the early success using Random Forest for default prediction, real-world records often behaves differently from curated data, and a later study **!!(\"Peer Lending Risk Predictor\" [3])!!** presented that a modified Logistic Regression model could outperform SVM, Naive Bayes, and even Random Forest on Lending Club data. \n",
    "\n",
    "The fact that Logistic Regression performance could be immensely improved by simply adding penalty factor on misclassification gave rise to our interest in fine tuning other not-yet optimized models, in particular, SVM and Naive Bayes, to continue the search for a better predictive model in the realm of loan default. Besides the difference in types of model that they focus on, the prior studies only used the out-of-the-box dataset from Kaggle or\n",
    "Lending Club, but research like **!!(\"The sensitivity of the loss given default rate to systematic risk\" [4])!!** has shown the linkage between default rate and macroeconomic factors, so we have decided to add in census data, with info like regional median income, to train our models on a more holistic set of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the data files have fields that contain NAs for older time periods. In order to collapse the data sets into one file, all numerical data will be stored in float fields (integer fields do not support NA missing values). To do this, we first define a conversion dictionary that stores the numeric fields with lookups to the *float* data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "convert_float = dict([s, float] for s in\n",
    "                     ['loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'installment',\n",
    "                      'annual_inc', 'dti', 'delinq_2yrs', 'inq_last_6mths', 'mths_since_last_delinq',                      \n",
    "                      'mths_since_last_record', 'open_acc', 'pub_rec', 'revol_bal', 'total_acc',\n",
    "                      'out_prncp', 'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp',\n",
    "                      'total_rec_int', 'total_rec_late_fee', 'recoveries', 'collection_recovery_fee',\n",
    "                      'last_pymnt_amnt', 'collections_12_mths_ex_med', 'mths_since_last_major_derog',\n",
    "                      'annual_inc_joint', 'dti_joint', 'acc_now_delinq', 'tot_coll_amt', 'tot_cur_bal', \n",
    "                      'open_acc_6m', 'open_il_6m', 'open_il_12m', 'open_il_24m', 'mths_since_rcnt_il', \n",
    "                      'total_bal_il', 'il_util', 'open_rv_12m', 'open_rv_24m', 'max_bal_bc', \n",
    "                      'all_util', 'total_rev_hi_lim', 'inq_fi', 'total_cu_tl', 'inq_last_12m', \n",
    "                      'acc_open_past_24mths', 'avg_cur_bal', 'bc_open_to_buy', 'bc_util',\n",
    "                      'chargeoff_within_12_mths', 'delinq_amnt', 'mo_sin_old_il_acct',\n",
    "                      'mo_sin_old_rev_tl_op', 'mo_sin_rcnt_rev_tl_op', 'mo_sin_rcnt_tl', 'mort_acc',\n",
    "                      'mths_since_recent_bc', 'mths_since_recent_bc_dlq', 'mths_since_recent_inq',\n",
    "                      'mths_since_recent_revol_delinq', 'num_accts_ever_120_pd', 'num_actv_bc_tl', \n",
    "                      'num_actv_rev_tl', 'num_bc_sats', 'num_bc_tl', 'num_il_tl', 'num_op_rev_tl',\n",
    "                      'num_rev_accts', 'num_rev_tl_bal_gt_0', 'num_sats', 'num_tl_120dpd_2m',\n",
    "                      'num_tl_30dpd', 'num_tl_90g_dpd_24m', 'num_tl_op_past_12m', 'pct_tl_nvr_dlq',\n",
    "                      'percent_bc_gt_75', 'pub_rec_bankruptcies', 'tax_liens', 'tot_hi_cred_lim',\n",
    "                      'total_bal_ex_mort', 'total_bc_limit', 'total_il_high_credit_limit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a dictionary of string fields, to handle situations where the inferred data type might be numeric even though the field should be read in as a string/object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convert_str = dict([s, str] for s in\n",
    "                    ['term', 'int_rate', 'grade', 'sub_grade', 'emp_title', 'emp_length', 'home_ownership', \n",
    "                     'verification_status', 'issue_d', 'loan_status', 'pymnt_plan', 'url', 'desc', \n",
    "                     'purpose', 'title', 'zip_code', 'addr_state', 'earliest_cr_line', 'revol_util', \n",
    "                     'initial_list_status', 'last_pymnt_d', 'next_pymnt_d', 'last_credit_pull_d',\n",
    "                     'policy_code', 'application_type', 'verification_status_joint'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a dictionary of string fields, to handle situations where the inferred data type might be numeric even though the field should be read in as a string/object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "convert_categ = dict([s, \"categorical\"] for s in\n",
    "                      [\"term\", \"grade\", \"sub_grade\", \"home_ownership\", \"emp_length\",\n",
    "                       \"verification_status\", \"loan_status\", \"pymnt_plan\", \"purpose\", \"addr_state\",\n",
    "                       \"initial_list_status\", \"policy_code\", \"application_type\", \"verification_status_joint\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the input data from the CSV data files using pandas *read_csv*. There is a blank row in the data header and there are two blank rows in the footer of each file. To allow the use of *skip_footer*, we use the python engine rather than the C engine. The first two columns (*id* and *member_id*) are unique and used to create a table index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_data(file_loc):\n",
    "    data = pd.read_csv(file_loc,\n",
    "                       skiprows=1,\n",
    "                       skip_footer=2,\n",
    "                       engine=\"python\",\n",
    "                       na_values=['NaN', 'nan'],\n",
    "                       converters=convert_str,\n",
    "                       index_col=[0,1])\n",
    "    \n",
    "    # Depending on the specific dataset used, the numeric values may be read in as integers.\n",
    "    # For best performance and to enable mergining of the datasets, we convert those fields\n",
    "    # to floats (which allow NaN values):\n",
    "    for k, v in convert_float.items():\n",
    "        data[k] = data[k].astype(v)\n",
    "    \n",
    "    # There are 5 object fields that contain dates in the format *YYYY-MMM* (e.g. '2010-Jan').\n",
    "    # We parse those to return datetime fields, which are more easily input into time series models\n",
    "    # or plotted in charts.\n",
    "    data.issue_d = pd.to_datetime(data.issue_d, errors=\"coerce\")\n",
    "    data.last_pymnt_d = pd.to_datetime(data.last_pymnt_d, errors=\"coerce\")\n",
    "    data.next_pymnt_d = pd.to_datetime(data.next_pymnt_d, errors=\"coerce\")\n",
    "    data.last_credit_pull_d = pd.to_datetime(data.last_credit_pull_d, errors=\"coerce\")\n",
    "    data.earliest_cr_line = pd.to_datetime(data.earliest_cr_line, errors=\"coerce\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = pd.concat([read_data(s) for s in [\"./data/LoanStats3a.csv\", \"./data/LoanStats3b.csv\", \"./data/LoanStats3c.csv\", \"./data/LoanStats3d.csv\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(887441, 109)\n"
     ]
    }
   ],
   "source": [
    "print data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the remaining fields contain categorical data. We use the pandas *category* data type to store the data more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k, v in convert_categ.items():\n",
    "    data[k] = pd.Categorical(data[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some percentages are stored as strings (*int_rate*, *revol_util*). Here we convert them into a float by stripping the % symbol and dividing by 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def percent_to_float(s):\n",
    "    if (type(s) == str):\n",
    "        if (\"%\" in s):\n",
    "            return float(str(s).strip(\"%\"))/100\n",
    "        else:\n",
    "            if s == \"None\":\n",
    "                return np.nan\n",
    "            else:            \n",
    "                return s\n",
    "    else:\n",
    "        return s\n",
    "\n",
    "data.int_rate = [percent_to_float(s) for s in data.int_rate]\n",
    "data.revol_util = [percent_to_float(s) for s in data.revol_util]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The remaining 5 object type fields are dropped as they aren't easily incorporated in a design matrix\n",
    "data = data.drop([\"emp_title\", \"url\", \"desc\", \"title\", \"zip_code\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6918350cd2f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# We decided to convert the date fields to the number of days since 1/1/1900\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdate_col\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"issue_d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"earliest_cr_line\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"last_pymnt_d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"next_pymnt_d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"last_credit_pull_d\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_col\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_col\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1/1/1900\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/cs109a_proj/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_has_valid_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/cs109a_proj/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;31m# maybe partial set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0mtake_split_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_mixed_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;31m# if there is only one block/type, still have to take split path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/cs109a_proj/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m_is_mixed_type\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2754\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_mixed_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2755\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_mixed_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2756\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2758\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/cs109a_proj/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m_protect_consolidate\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m   2716\u001b[0m         \"\"\"\n\u001b[1;32m   2717\u001b[0m         \u001b[0mblocks_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2718\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2719\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mblocks_before\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2720\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/cs109a_proj/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2753\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2754\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_mixed_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2755\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_mixed_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2756\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/cs109a_proj/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mis_mixed_type\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3010\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mis_mixed_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3011\u001b[0m         \u001b[0;31m# Warning, consolidation needs to get checked upstairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3012\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3013\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/cs109a_proj/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3277\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_consolidated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3278\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3279\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3280\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_known_consolidated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/cs109a_proj/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36m_consolidate\u001b[0;34m(blocks)\u001b[0m\n\u001b[1;32m   4267\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_blocks\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4268\u001b[0m         merged_blocks = _merge_blocks(list(group_blocks), dtype=dtype,\n\u001b[0;32m-> 4269\u001b[0;31m                                       _can_consolidate=_can_consolidate)\n\u001b[0m\u001b[1;32m   4270\u001b[0m         \u001b[0mnew_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_blocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4271\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/cs109a_proj/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36m_merge_blocks\u001b[0;34m(blocks, dtype, _can_consolidate)\u001b[0m\n\u001b[1;32m   4290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4291\u001b[0m         \u001b[0margsort\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_mgr_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4292\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4293\u001b[0m         \u001b[0mnew_mgr_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# We decided to convert the date fields to the number of days since 1/1/1900\n",
    "for date_col in [\"issue_d\", \"earliest_cr_line\", \"last_pymnt_d\", \"next_pymnt_d\", \"last_credit_pull_d\"]:\n",
    "    data.loc[:, date_col] = (data.loc[:, date_col] - pd.to_datetime(\"1/1/1900\")).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current                                                467679\n",
      "Fully Paid                                             315192\n",
      "Charged Off                                             75740\n",
      "Late (31-120 days)                                      14548\n",
      "In Grace Period                                          8432\n",
      "Late (16-30 days)                                        2968\n",
      "Does not meet the credit policy. Status:Fully Paid       1988\n",
      "Does not meet the credit policy. Status:Charged Off       761\n",
      "Default                                                   132\n",
      "None                                                        1\n",
      "Name: loan_status, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Let us see what status we have.\n",
    "print data[\"loan_status\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(391064, 104)\n",
      "0    315192\n",
      "1     75872\n",
      "Name: loan_status, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/cs109a_proj/lib/python2.7/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/ubuntu/anaconda3/envs/cs109a_proj/lib/python2.7/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/ubuntu/anaconda3/envs/cs109a_proj/lib/python2.7/site-packages/ipykernel/__main__.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# Ok we dont want current or None status as we don't know what is going to happen with those.\n",
    "# next also late and grace period we are not sure about.\n",
    "# thirdly we remove loans that do not meet credit policy so we can convert our issue to a binar model\n",
    "data = data[((data[\"loan_status\"] == \"Default\") | (data[\"loan_status\"] == \"Charged Off\") | (data[\"loan_status\"] == \"Fully Paid\"))]\n",
    "\n",
    "# Rename \"Charged off to default\"\n",
    "data[\"loan_status\"] = data[\"loan_status\"].replace(\"Charged Off\", \"Default\")\n",
    "data[\"loan_status\"] = data[\"loan_status\"].replace(\"Default\", 1)\n",
    "data[\"loan_status\"] = data[\"loan_status\"].replace(\"Fully Paid\", 0)\n",
    "print data.shape\n",
    "print data[\"loan_status\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/cs109a_proj/lib/python2.7/site-packages/pandas/core/indexing.py:465: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "# There are only 50 joint applications, so we coalesce the fields that have joint values (annual_inc, dti, verification_status)\n",
    "for joint_col in [\"annual_inc\", \"dti\", \"verification_status\"]:\n",
    "    data.loc[data.application_type==\"Joint\", joint_col] = data.loc[data.application_type==\"Joint\", joint_col + \"_joint\"]\n",
    "\n",
    "data = data.drop([\"annual_inc_joint\", \"dti_joint\", \"verification_status_joint\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# policy_code has only one level, \"1\", and pymnt_plan is all \"N\" except for 4 records so we drop both \n",
    "data = data.drop([\"pymnt_plan\", \"policy_code\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# While grade and sub_grade are thought to be predictive, they use Lending Tree's credit model and we don't\n",
    "# want to influence our scoring model using their model that will capture much of the same information\n",
    "# This is also true of int_rate and installment, which are each correlated with LT's credit grade\n",
    "data = data.drop([\"int_rate\", \"installment\", \"grade\", \"sub_grade\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finally we drop other fields that are not known a priori and may contain information\n",
    "# post-default / delinquency that will bias our results\n",
    "# This includes payment history and loan balances\n",
    "data = data.drop([\"out_prncp\", \"out_prncp_inv\", \"total_pymnt\", \"total_pymnt_inv\", \"total_rec_prncp\",\n",
    "                  \"total_rec_int\", \"total_rec_late_fee\", \"recoveries\", \"collection_recovery_fee\",\n",
    "                  \"last_pymnt_d\", \"last_pymnt_amnt\", \"next_pymnt_d\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import quandl\n",
    "state_list = [\"AK\", \"AL\", \"AR\", \"AZ\", \"CA\", \"CO\", \"CT\", \"DC\", \"DE\", \"FL\",\n",
    "              \"GA\", \"HI\", \"IA\", \"ID\", \"IL\", \"IN\", \"KS\", \"KY\", \"LA\", \"MA\",\n",
    "              \"MD\", \"ME\", \"MI\", \"MN\", \"MO\", \"MS\", \"MT\", \"NC\", \"ND\", \"NE\",\n",
    "              \"NH\", \"NJ\", \"NM\", \"NV\", \"NY\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\",\n",
    "              \"SC\", \"SD\", \"TN\", \"TX\", \"UT\", \"VA\", \"VT\", \"WA\", \"WI\", \"WV\",\n",
    "              \"WY\"]\n",
    "#state_unemployment_data = {}\n",
    "#for i in state_list:\n",
    "#    try:\n",
    "#        print(i)\n",
    "#        result = quandl.get('FRED/{:s}UR'.format(i))\n",
    "#        state_unemployment_data[i] = result\n",
    "#    except RuntimeError as e:\n",
    "#        print(e)\n",
    "\n",
    "#check all indices are sorted by date\n",
    "#for s in state_unemployment_data.keys():\n",
    "#    if not state_unemployment_data[s].index.is_monotonic:\n",
    "#        print s\n",
    "\n",
    "#convert percentages to a value in the range [0, 1]\n",
    "#for s in state_unemployment_data.keys():\n",
    "#    state_unemployment_data[s]['VALUE'] = state_unemployment_data[s]['VALUE'] / 100.0\n",
    "\n",
    "#import pickle\n",
    "#with open('unemployment_rates_by_state.pkl', 'wb') as f:\n",
    "#    pickle.dump(state_unemployment_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('unemployment_rates_by_state.pkl', 'rb') as f:\n",
    "    state_unemployment_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_state_unemp_df(s):\n",
    "    unemp_data = state_unemployment_data[s]\n",
    "    unemp_data = unemp_data.rename(columns={'VALUE':s})\n",
    "    return unemp_data\n",
    "\n",
    "df_unemp = pd.concat([get_state_unemp_df(s) for s in state_list], axis=1)\n",
    "df_unemp.index = (pd.to_datetime(df_unemp.index) - pd.to_datetime(\"1/1/1900\")).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lookup_unemp(issue_date, state):\n",
    "    try:\n",
    "        result = df_unemp.loc[issue_date, state]\n",
    "    except:\n",
    "        result = np.na\n",
    "    return result\n",
    "\n",
    "data.state_unemp_rate = [lookup_unemp(issue_date, state) for issue_date, state in zip(data.issue_d, data.addr_state)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_y = data[\"loan_status\"]\n",
    "df_X = data.drop(\"loan_status\", axis=1)\n",
    "# Remove member id.\n",
    "df_X = df_X.ix[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print df_X.shape\n",
    "print df_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k, v in convert_categ.items():\n",
    "    if not(k in [\"verification_status_joint\", \"policy_code\", \"grade\", \"sub_grade\", \"pymnt_plan\", \"loan_status\"]):\n",
    "        df_X = pd.concat([df_X,\n",
    "                          pd.get_dummies(df_X[k], prefix=k)],\n",
    "                         axis=1)\n",
    "        df_X = df_X.drop(k, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_X.to_pickle(\"./data/df_X.pkl\")\n",
    "#df_y.to_pickle(\"./data/df_y.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./data/df_X.pkl', 'rb') as f:\n",
    "    df_X = pickle.load(f)\n",
    "\n",
    "with open('./data/df_y.pkl', 'rb') as f:\n",
    "    df_y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!! We are deleting these columns based on the high number of nulls, but they seem like they may contain valuable information. Time since delinquency for example should be quite predictive of future defaults. Leaving these columns intact for the time being!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# There is only so much we can do, delete features that have more than 40% \n",
    "# of data as nan.\n",
    "for col in df_X.columns:\n",
    "    if (df_X[col].isnull().sum()/float(df_X.shape[0])) > 0.4:\n",
    "        #df_X = df_X.drop(col, axis=1)\n",
    "        print col\n",
    "#print df_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!!Including this section from Final_Project_MASTER!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!!Including this section from Final_Project_MASTER!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default/Paid ratio: 19.4%\n"
     ]
    }
   ],
   "source": [
    "print 'Default/Paid ratio: %.1f%%' % (np.mean(df_y)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def impute_mean(X_train, X_test):\n",
    "#    imp = Imputer(missing_values='NaN', strategy='mean', axis=0, copy=True)\n",
    "#    imp.fit(X_train)\n",
    "#    return imp.transform(X_train), imp.transform(X_test)\n",
    "\n",
    "#def impute_knn(X_train, X_test):\n",
    "    # NOT IMPLEMENTED\n",
    "#    knn = NearestNeighbors(n_neighbors=5)\n",
    "#    knn.fit(X_train)\n",
    "#    return knn.predict(X_train), knn.predict(X_test)\n",
    "\n",
    "#X_train, X_test = impute_mean(df_X_train, df_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#modified version of CV optimize from Lecture 17\n",
    "def cv_optimize(clf, parameters, X, y, n_jobs=1, n_folds=5, score_func=None):\n",
    "    if score_func:\n",
    "        gs = GridSearchCV(clf, param_grid=parameters, cv=n_folds, n_jobs=n_jobs, scoring=score_func)\n",
    "    else:\n",
    "        gs = GridSearchCV(clf, param_grid=parameters, n_jobs=n_jobs, cv=n_folds)\n",
    "    gs.fit(X, y)\n",
    "\n",
    "    best = gs.best_estimator_\n",
    "    return best, gs\n",
    "\n",
    "def drop_column(df_in, col):\n",
    "    df_out = df_in[df_in.columns[df_in.columns!=col]]\n",
    "    return df_out\n",
    "\n",
    "def drop_columns(df_in, cols):\n",
    "    X_new = df_in.copy()\n",
    "    for c in cols:\n",
    "        X_new = drop_column(X_new, c)\n",
    "    return X_new\n",
    "\n",
    "def impute_mean(df_in, col):\n",
    "    mean_val = df_in[~df_in[col].isnull()][col].mean()\n",
    "    df_in.loc[df_in[col].isnull(), col] = mean_val\n",
    "\n",
    "def impute_knn(df, col):\n",
    "    df_temp = df.copy()\n",
    "    for c in df_temp.columns:\n",
    "        if c != col and df_temp[c].isnull().sum() > 0:\n",
    "            impute_mean(df_temp, c)\n",
    "    knn = KNeighborsRegressor()\n",
    "    X_imp_train = drop_column(df_temp, col)\n",
    "    X = X_imp_train[~df[col].isnull()]\n",
    "    y_imp_train = df[col]\n",
    "    y = y_imp_train[~df[col].isnull()]\n",
    "    #best, gs = cv_optimize(knn, {'n_neighbors':[3, 5, 10]}, X, y, n_jobs = -1)\n",
    "    best, gs = cv_optimize(knn, {'n_neighbors':[5]}, X, y, n_jobs = -1)\n",
    "    X_pred = X_imp_train[df[col].isnull()]\n",
    "    df.loc[df[col].isnull(),col] = best.predict(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mths_since_last_delinq\n"
     ]
    }
   ],
   "source": [
    "for c in df_X.columns:\n",
    "    if df_X[c].isnull().sum() > 0:\n",
    "        print c\n",
    "        impute_knn(df_X, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split into training and test data\n",
    "df_X_train, df_X_test, df_y_train, df_y_test = train_test_split(df_X, df_y, test_size=0.3, random_state=20161201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scale_std(X_train, X_test, n_col):\n",
    "    sclr = StandardScaler()\n",
    "    sclr.fit(X_train)\n",
    "    X_train_sc = sclr.transform(X_train)\n",
    "    X_test_sc = sclr.transform(X_test)\n",
    "    X_train_sc[:, n_col:] = X_train[:, n_col:]\n",
    "    X_test_sc[:, n_col:] = X_test[:, n_col:]\n",
    "    return X_train_sc, X_test_sc\n",
    "\n",
    "# Only scale the first 74 columns (the remaining columns are one-hot encoded)\n",
    "X_train, X_test = scale_std(X_train, X_test, 74)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_X_train_pp = pd.DataFrame(X_train, index=df_X_train.index, columns=df_X_train.columns)\n",
    "df_X_test_pp = pd.DataFrame(X_test, index=df_X_test.index, columns=df_X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = np.array(df_X_train_pp)\n",
    "X_test = np.array(df_X_test_pp)\n",
    "y_train = np.array(df_y_train)\n",
    "y_test = np.array(df_y_test)\n",
    "np.save(\"./data/X_train.npy\", X_train)\n",
    "np.save(\"./data/X_test.npy\", X_test)\n",
    "np.save(\"./data/y_train.npy\", y_train)\n",
    "np.save(\"./data/y_test.npy\", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature 15 (0.069455) funded_amnt\n",
      "2. feature 77 (0.021385) funded_amnt_inv\n",
      "3. feature 4 (0.021021) annual_inc\n",
      "4. feature 1 (0.020174) issue_d\n",
      "5. feature 3 (0.018659) dti\n",
      "6. feature 2 (0.018423) delinq_2yrs\n",
      "7. feature 13 (0.018340) earliest_cr_line\n",
      "8. feature 6 (0.016751) inq_last_6mths\n",
      "9. feature 14 (0.016593) mths_since_last_delinq\n",
      "10. feature 39 (0.016333) mths_since_last_record\n",
      "11. feature 0 (0.016275) open_acc\n",
      "12. feature 12 (0.016188) pub_rec\n",
      "13. feature 10 (0.015872) revol_bal\n",
      "14. feature 36 (0.015455) revol_util\n",
      "15. feature 38 (0.014689) total_acc\n",
      "16. feature 66 (0.014384) last_credit_pull_d\n",
      "17. feature 49 (0.014084) collections_12_mths_ex_med\n",
      "18. feature 42 (0.014006) mths_since_last_major_derog\n",
      "19. feature 71 (0.013809) acc_now_delinq\n",
      "20. feature 8 (0.013783) tot_coll_amt\n",
      "21. feature 70 (0.013583) tot_cur_bal\n",
      "22. feature 56 (0.013450) open_acc_6m\n",
      "23. feature 43 (0.013443) open_il_6m\n",
      "24. feature 60 (0.013354) open_il_12m\n",
      "25. feature 55 (0.013172) open_il_24m\n",
      "26. feature 45 (0.013107) mths_since_rcnt_il\n",
      "27. feature 72 (0.012977) total_bal_il\n",
      "28. feature 7 (0.012883) il_util\n",
      "29. feature 64 (0.012805) open_rv_12m\n",
      "30. feature 37 (0.012712) open_rv_24m\n",
      "31. feature 54 (0.012607) max_bal_bc\n",
      "32. feature 59 (0.012535) all_util\n",
      "33. feature 69 (0.012513) total_rev_hi_lim\n",
      "34. feature 58 (0.012405) inq_fi\n",
      "35. feature 47 (0.012219) total_cu_tl\n",
      "36. feature 20 (0.012057) inq_last_12m\n",
      "37. feature 44 (0.012051) acc_open_past_24mths\n",
      "38. feature 32 (0.011746) avg_cur_bal\n",
      "39. feature 52 (0.011617) bc_open_to_buy\n",
      "40. feature 65 (0.011581) bc_util\n",
      "41. feature 57 (0.011554) chargeoff_within_12_mths\n",
      "42. feature 53 (0.011298) delinq_amnt\n",
      "43. feature 46 (0.011262) mo_sin_old_il_acct\n",
      "44. feature 50 (0.009751) mo_sin_old_rev_tl_op\n",
      "45. feature 5 (0.008123) mo_sin_rcnt_rev_tl_op\n",
      "46. feature 51 (0.008091) mo_sin_rcnt_tl\n",
      "47. feature 102 (0.007936) mort_acc\n",
      "48. feature 17 (0.007838) mths_since_recent_bc\n",
      "49. feature 48 (0.007778) mths_since_recent_bc_dlq\n",
      "50. feature 9 (0.007267) mths_since_recent_inq\n",
      "51. feature 125 (0.007180) mths_since_recent_revol_delinq\n",
      "52. feature 19 (0.007160) num_accts_ever_120_pd\n",
      "53. feature 120 (0.006323) num_actv_bc_tl\n",
      "54. feature 11 (0.006274) num_actv_rev_tl\n",
      "55. feature 155 (0.006265) num_bc_sats\n",
      "56. feature 67 (0.006243) num_bc_tl\n",
      "57. feature 130 (0.006194) num_il_tl\n",
      "58. feature 84 (0.006157) num_op_rev_tl\n",
      "59. feature 85 (0.006013) num_rev_accts\n",
      "60. feature 78 (0.005828) num_rev_tl_bal_gt_0\n",
      "61. feature 103 (0.005799) num_sats\n",
      "62. feature 76 (0.005718) num_tl_120dpd_2m\n",
      "63. feature 89 (0.005612) num_tl_30dpd\n",
      "64. feature 165 (0.005335) num_tl_90g_dpd_24m\n",
      "65. feature 104 (0.005325) num_tl_op_past_12m\n",
      "66. feature 101 (0.004865) pct_tl_nvr_dlq\n",
      "67. feature 106 (0.004848) percent_bc_gt_75\n",
      "68. feature 111 (0.004842) pub_rec_bankruptcies\n",
      "69. feature 105 (0.004653) tax_liens\n",
      "70. feature 108 (0.004476) tot_hi_cred_lim\n",
      "71. feature 63 (0.004233) total_bal_ex_mort\n",
      "72. feature 135 (0.004125) total_bc_limit\n",
      "73. feature 75 (0.004020) total_il_high_credit_limit\n",
      "74. feature 157 (0.004015) verification_status_None\n",
      "75. feature 167 (0.003981) verification_status_Not Verified\n",
      "76. feature 109 (0.003976) verification_status_Source Verified\n",
      "77. feature 160 (0.003971) verification_status_Verified\n",
      "78. feature 152 (0.003891) term_ 36 months\n",
      "79. feature 107 (0.003865) term_ 60 months\n",
      "80. feature 96 (0.003786) term_None\n",
      "81. feature 110 (0.003706) application_type_INDIVIDUAL\n",
      "82. feature 131 (0.003623) application_type_JOINT\n",
      "83. feature 115 (0.003448) application_type_None\n",
      "84. feature 143 (0.003405) initial_list_status_None\n",
      "85. feature 61 (0.003285) initial_list_status_f\n",
      "86. feature 88 (0.003270) initial_list_status_w\n",
      "87. feature 91 (0.003179) purpose_None\n",
      "88. feature 141 (0.003107) purpose_car\n",
      "89. feature 140 (0.003065) purpose_credit_card\n",
      "90. feature 124 (0.003015) purpose_debt_consolidation\n",
      "91. feature 144 (0.002951) purpose_educational\n",
      "92. feature 148 (0.002946) purpose_home_improvement\n",
      "93. feature 154 (0.002908) purpose_house\n",
      "94. feature 126 (0.002896) purpose_major_purchase\n",
      "95. feature 119 (0.002835) purpose_medical\n",
      "96. feature 98 (0.002719) purpose_moving\n",
      "97. feature 74 (0.002675) purpose_other\n",
      "98. feature 169 (0.002667) purpose_renewable_energy\n",
      "99. feature 145 (0.002633) purpose_small_business\n",
      "100. feature 136 (0.002513) purpose_vacation\n",
      "\"funded_amnt\",\n",
      "\"funded_amnt_inv\",\n",
      "\"annual_inc\",\n",
      "\"issue_d\",\n",
      "\"dti\",\n",
      "\"delinq_2yrs\",\n",
      "\"earliest_cr_line\",\n",
      "\"inq_last_6mths\",\n",
      "\"mths_since_last_delinq\",\n",
      "\"mths_since_last_record\",\n",
      "\"open_acc\",\n",
      "\"pub_rec\",\n",
      "\"revol_bal\",\n",
      "\"revol_util\",\n",
      "\"total_acc\",\n",
      "\"last_credit_pull_d\",\n",
      "\"collections_12_mths_ex_med\",\n",
      "\"mths_since_last_major_derog\",\n",
      "\"acc_now_delinq\",\n",
      "\"tot_coll_amt\",\n",
      "\"tot_cur_bal\",\n",
      "\"open_acc_6m\",\n",
      "\"open_il_6m\",\n",
      "\"open_il_12m\",\n",
      "\"open_il_24m\",\n",
      "\"mths_since_rcnt_il\",\n",
      "\"total_bal_il\",\n",
      "\"il_util\",\n",
      "\"open_rv_12m\",\n",
      "\"open_rv_24m\",\n",
      "\"max_bal_bc\",\n",
      "\"all_util\",\n",
      "\"total_rev_hi_lim\",\n",
      "\"inq_fi\",\n",
      "\"total_cu_tl\",\n",
      "\"inq_last_12m\",\n",
      "\"acc_open_past_24mths\",\n",
      "\"avg_cur_bal\",\n",
      "\"bc_open_to_buy\",\n",
      "\"bc_util\",\n",
      "\"chargeoff_within_12_mths\",\n",
      "\"delinq_amnt\",\n",
      "\"mo_sin_old_il_acct\",\n",
      "\"mo_sin_old_rev_tl_op\",\n",
      "\"mo_sin_rcnt_rev_tl_op\",\n",
      "\"mo_sin_rcnt_tl\",\n",
      "\"mort_acc\",\n",
      "\"mths_since_recent_bc\",\n",
      "\"mths_since_recent_bc_dlq\",\n",
      "\"mths_since_recent_inq\",\n",
      "\"mths_since_recent_revol_delinq\",\n",
      "\"num_accts_ever_120_pd\",\n",
      "\"num_actv_bc_tl\",\n",
      "\"num_actv_rev_tl\",\n",
      "\"num_bc_sats\",\n",
      "\"num_bc_tl\",\n",
      "\"num_il_tl\",\n",
      "\"num_op_rev_tl\",\n",
      "\"num_rev_accts\",\n",
      "\"num_rev_tl_bal_gt_0\",\n",
      "\"num_sats\",\n",
      "\"num_tl_120dpd_2m\",\n",
      "\"num_tl_30dpd\",\n",
      "\"num_tl_90g_dpd_24m\",\n",
      "\"num_tl_op_past_12m\",\n",
      "\"pct_tl_nvr_dlq\",\n",
      "\"percent_bc_gt_75\",\n",
      "\"pub_rec_bankruptcies\",\n",
      "\"tax_liens\",\n",
      "\"tot_hi_cred_lim\",\n",
      "\"total_bal_ex_mort\",\n",
      "\"total_bc_limit\",\n",
      "\"total_il_high_credit_limit\",\n",
      "\"verification_status_None\",\n",
      "\"verification_status_Not Verified\",\n",
      "\"verification_status_Source Verified\",\n",
      "\"verification_status_Verified\",\n",
      "\"term_ 36 months\",\n",
      "\"term_ 60 months\",\n",
      "\"term_None\",\n",
      "\"application_type_INDIVIDUAL\",\n",
      "\"application_type_JOINT\",\n",
      "\"application_type_None\",\n",
      "\"initial_list_status_None\",\n",
      "\"initial_list_status_f\",\n",
      "\"initial_list_status_w\",\n",
      "\"purpose_None\",\n",
      "\"purpose_car\",\n",
      "\"purpose_credit_card\",\n",
      "\"purpose_debt_consolidation\",\n",
      "\"purpose_educational\",\n",
      "\"purpose_home_improvement\",\n",
      "\"purpose_house\",\n",
      "\"purpose_major_purchase\",\n",
      "\"purpose_medical\",\n",
      "\"purpose_moving\",\n",
      "\"purpose_other\",\n",
      "\"purpose_renewable_energy\",\n",
      "\"purpose_small_business\",\n",
      "\"purpose_vacation\",\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-89eaa7059614>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# We ran this test on multiple small subsets of data and got similar results.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mfeature_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_X_train_pp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_y_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-89eaa7059614>\u001b[0m in \u001b[0;36mfeature_plot\u001b[0;34m(X, y, estimators, features_to_print)\u001b[0m\n\u001b[1;32m     26\u001b[0m            color=\"r\", yerr=std[indices], align=\"center\")\n\u001b[1;32m     27\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_to_print\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_to_print\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/cs109a_proj/lib/python2.7/site-packages/matplotlib/pyplot.pyc\u001b[0m in \u001b[0;36mxlim\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1583\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_xlim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1585\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1586\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/cs109a_proj/lib/python2.7/site-packages/matplotlib/axes/_base.pyc\u001b[0m in \u001b[0;36mset_xlim\u001b[0;34m(self, left, right, emit, auto, **kw)\u001b[0m\n\u001b[1;32m   2781\u001b[0m                  \u001b[0;34m'in singular transformations; automatically expanding.\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2782\u001b[0m                  'left=%s, right=%s') % (left, right))\n\u001b[0;32m-> 2783\u001b[0;31m         \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonsingular\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincreasing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2784\u001b[0m         \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit_range_for_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/cs109a_proj/lib/python2.7/site-packages/matplotlib/transforms.pyc\u001b[0m in \u001b[0;36mnonsingular\u001b[0;34m(vmin, vmax, expander, tiny, increasing)\u001b[0m\n\u001b[1;32m   2749\u001b[0m     \u001b[0mclose\u001b[0m \u001b[0mto\u001b[0m \u001b[0mzero\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mexpander\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mexpander\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2750\u001b[0m     '''\n\u001b[0;32m-> 2751\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mexpander\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpander\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "def feature_plot(X, y, estimators, features_to_print):\n",
    "    # Build a forest and compute the feature importances\n",
    "    forest = ExtraTreesClassifier(n_estimators=estimators,\n",
    "                                  random_state=0)\n",
    "\n",
    "    forest.fit(X, y)\n",
    "    importances = forest.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "                 axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # Print the feature ranking\n",
    "    print(\"Feature ranking:\")\n",
    "\n",
    "    for f in range(features_to_print):\n",
    "        print(\"%d. feature %d (%f) %s\" % (f + 1, indices[f], importances[indices[f]], X.columns[f]))\n",
    "    \n",
    "    for f in range(features_to_print):\n",
    "        print '\"' + str(X.columns[f]) + '\",'\n",
    "    \n",
    "    # Plot the feature importances of the forest\n",
    "    plt.figure()\n",
    "    plt.title(\"Feature importances\")\n",
    "    indices = indices[0:features_to_print]\n",
    "    plt.bar(range(features_to_print), importances[indices],\n",
    "           color=\"r\", yerr=std[indices], align=\"center\")\n",
    "    plt.xticks(range(features_to_print), indices)\n",
    "    plt.xlim([-1, range(features_to_print)])\n",
    "    plt.show()\n",
    "\n",
    "# We ran this test on multiple small subsets of data and got similar results.\n",
    "feature_plot(df_X_train_pp, df_y_train, 1, 100)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:cs109a_proj]",
   "language": "python",
   "name": "conda-env-cs109a_proj-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
