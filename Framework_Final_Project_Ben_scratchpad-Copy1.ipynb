{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "**<font size=7><center>Predicting Default Rates for Lending Club</center></font>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>Devon Luongo</font> <br>\n",
    "<font size=4>Harvard University</font> <br>\n",
    "<font size=3>deluongo@gmail.com</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>Ben Yuen</font> <br>\n",
    "<font size=4>Harvard University</font> <br>\n",
    "<font size=3></font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>Bryn Clark</font> <br>\n",
    "<font size=4>Harvard University</font> <br>\n",
    "<font size=3></font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>Ankit Agarwal</font> <br>\n",
    "<font size=4>Harvard University</font> <br>\n",
    "<font size=3></font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font size=4><center>Abstract</center></font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>**!!(Insert Abstract)!!**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lending Club is an online marketplace that fascilitates the lending money to individuals or businesses through online services that match lenders directly with borrowers. This practice is called Peer-to-Peer (P2P) Lending. With an A+ BBB rating, Lending Club offers an attractive alternative to bonds for steady investment income. Lending Club also states that borrowers are able to reduce their loans by an average of 30%.\n",
    "\n",
    "\"Since peer-to-peer lending companies offering these services operate entirely online, they can run with lower overhead and provide the service more cheaply than traditional financial institutions. As a result, lenders often earn higher returns compared to savings and investment products offered by banks, while borrowers can borrow money at lower interest rates, even after the P2P lending company has taken a fee for providing the match-making platform and credit checking the borrower.\"\n",
    "\n",
    "The downside to Lending Club however, is that most peer-to-peer loans are unsecured personal loans, meaning lenders incur a higher risk than comparable steady income investments like Treasury Bonds. Therefore, the investment quality of each loan depends largly on three factors: the expected return, the level of risk aversion, and the risk incurred. The first factor is built into the contract of the loan, and the second factor is personal preference. Therefore, the primary variable that that investors must identify when compoaring P2P Lending with other investment opportunities is the default risk for each loan. \n",
    "\n",
    "For this reason, Lending Club offers lenders a dataset containing a comprehensive list of features that can be employed to make better lending decisions. Detailed information for every loan have been issued by Lending Club from 2007 to 2015. Among**!!(Insert Num Predictors)!!** features or predictors in total, the dataset includes a borrower’s annual incomes, zip codes, revolving balances, and purpose for borrowing.\n",
    "\n",
    "Our objective was to build on similar projects conducted in recent years to construct a model for default risk to help lenders decrease the risk they occur when lending. We trained and tested a range of models in an attempt to identify the best performing model. Additionally, we constructed a cost based model aimed at maximizing Lending Clubs overhead.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Related Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!!(Rewrite After Models Complete)!!** \n",
    "Prior projects like **!!(\"Predicting borrowers chance of defaulting on credit loans\" [1])!!** have set great examples of applying machine learning to improve loan default prediction in a Kaggle competition, and authors for**!!(\"Predicting Probability of Loan Default\" [2])!!**  have shown that Random Forest appeared to be the best performing model on the Kaggle data. \n",
    "\n",
    "However, despite the early success using Random Forest for default prediction, real-world records often behaves differently from curated data, and a later study **!!(\"Peer Lending Risk Predictor\" [3])!!** presented that a modified Logistic Regression model could outperform SVM, Naive Bayes, and even Random Forest on Lending Club data. \n",
    "\n",
    "The fact that Logistic Regression performance could be immensely improved by simply adding penalty factor on misclassification gave rise to our interest in fine tuning other not-yet optimized models, in particular, SVM and Naive Bayes, to continue the search for a better predictive model in the realm of loan default. Besides the difference in types of model that they focus on, the prior studies only used the out-of-the-box dataset from Kaggle or\n",
    "Lending Club, but research like **!!(\"The sensitivity of the loss given default rate to systematic risk\" [4])!!** has shown the linkage between default rate and macroeconomic factors, so we have decided to add in census data, with info like regional median income, to train our models on a more holistic set of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u> Libraries:</u> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from matplotlib.patches import Polygon\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as colors\n",
    "import scipy\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#new imports for milestone 4\n",
    "from io import StringIO\n",
    "from IPython.display import Image\n",
    "import pydotplus\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.metrics import accuracy_score, f1_score, make_scorer, roc_auc_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics  import confusion_matrix\n",
    "import itertools\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"/home/ankit/anaconda2/pkgs/basemap-1.0.7-np111py27_0/lib/python2.7/site-packages/mpl_toolkits/basemap/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the data files have fields that contain NAs for older time periods. In order to collapse the data sets into one file, all numerical data will be stored in float fields (integer fields do not support NA missing values). To do this, we first define a conversion dictionary that stores the numeric fields with lookups to the *float* data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "convert_float = dict([s, float] for s in\n",
    "                     ['loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'installment',\n",
    "                      'annual_inc', 'dti', 'delinq_2yrs', 'inq_last_6mths', 'mths_since_last_delinq',                      \n",
    "                      'mths_since_last_record', 'open_acc', 'pub_rec', 'revol_bal', 'total_acc',\n",
    "                      'out_prncp', 'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp',\n",
    "                      'total_rec_int', 'total_rec_late_fee', 'recoveries', 'collection_recovery_fee',\n",
    "                      'last_pymnt_amnt', 'collections_12_mths_ex_med', 'mths_since_last_major_derog',\n",
    "                      'annual_inc_joint', 'dti_joint', 'acc_now_delinq', 'tot_coll_amt', 'tot_cur_bal', \n",
    "                      'open_acc_6m', 'open_il_6m', 'open_il_12m', 'open_il_24m', 'mths_since_rcnt_il', \n",
    "                      'total_bal_il', 'il_util', 'open_rv_12m', 'open_rv_24m', 'max_bal_bc', \n",
    "                      'all_util', 'total_rev_hi_lim', 'inq_fi', 'total_cu_tl', 'inq_last_12m', \n",
    "                      'acc_open_past_24mths', 'avg_cur_bal', 'bc_open_to_buy', 'bc_util',\n",
    "                      'chargeoff_within_12_mths', 'delinq_amnt', 'mo_sin_old_il_acct',\n",
    "                      'mo_sin_old_rev_tl_op', 'mo_sin_rcnt_rev_tl_op', 'mo_sin_rcnt_tl', 'mort_acc',\n",
    "                      'mths_since_recent_bc', 'mths_since_recent_bc_dlq', 'mths_since_recent_inq',\n",
    "                      'mths_since_recent_revol_delinq', 'num_accts_ever_120_pd', 'num_actv_bc_tl', \n",
    "                      'num_actv_rev_tl', 'num_bc_sats', 'num_bc_tl', 'num_il_tl', 'num_op_rev_tl',\n",
    "                      'num_rev_accts', 'num_rev_tl_bal_gt_0', 'num_sats', 'num_tl_120dpd_2m',\n",
    "                      'num_tl_30dpd', 'num_tl_90g_dpd_24m', 'num_tl_op_past_12m', 'pct_tl_nvr_dlq',\n",
    "                      'percent_bc_gt_75', 'pub_rec_bankruptcies', 'tax_liens', 'tot_hi_cred_lim',\n",
    "                      'total_bal_ex_mort', 'total_bc_limit', 'total_il_high_credit_limit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a dictionary of string fields, to handle situations where the inferred data type might be numeric even though the field should be read in as a string/object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convert_str = dict([s, str] for s in\n",
    "                    ['term', 'int_rate', 'grade', 'sub_grade', 'emp_title', 'emp_length', 'home_ownership', \n",
    "                     'verification_status', 'issue_d', 'loan_status', 'pymnt_plan', 'url', 'desc', \n",
    "                     'purpose', 'title', 'zip_code', 'addr_state', 'earliest_cr_line', 'revol_util', \n",
    "                     'initial_list_status', 'last_pymnt_d', 'next_pymnt_d', 'last_credit_pull_d',\n",
    "                     'policy_code', 'application_type', 'verification_status_joint'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the input data from the CSV data files using pandas *read_csv*. There is a blank row in the data header and there are two blank rows in the footer of each file. To allow the use of *skip_footer*, we use the python engine rather than the C engine. The first two columns (*id* and *member_id*) are unique and used to create a table index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_2007_2011 = pd.read_csv(\"./data/LoanStats3a.csv\",\n",
    "                   skiprows=1,\n",
    "                   skip_footer=2,\n",
    "                   engine=\"python\",\n",
    "                   na_values=['NaN', 'nan'],\n",
    "                   converters=convert_str,\n",
    "                   index_col=[0,1])\n",
    "\n",
    "data_2014 = pd.read_csv(\"./data/LoanStats3b.csv\",\n",
    "                   skiprows=1,\n",
    "                   skip_footer=2,\n",
    "                   engine=\"python\",\n",
    "                   na_values=['NaN', 'nan'],\n",
    "                   converters=convert_str,\n",
    "                   index_col=[0,1])\n",
    "\n",
    "data_2015 = pd.read_csv(\"./data/LoanStats3c.csv\",\n",
    "                   skiprows=1,\n",
    "                   skip_footer=2,\n",
    "                   engine=\"python\",\n",
    "                   na_values=['NaN', 'nan'],\n",
    "                   converters=convert_str,\n",
    "                   index_col=[0,1])\n",
    "\n",
    "data = pd.read_csv(\"./data/LoanStats3d.csv\",\n",
    "                   skiprows=1,\n",
    "                   skip_footer=2,\n",
    "                   engine=\"python\",\n",
    "                   na_values=['NaN', 'nan'],\n",
    "                   converters=convert_str,\n",
    "                   index_col=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data table dimensions 2007-2011: 42536 x 109\n",
      "Data table dimensions 2014: 188181 x 109\n",
      "Data table dimensions 2015: 235629 x 109\n",
      "Data table dimensions 2016: 421095 x 109\n"
     ]
    }
   ],
   "source": [
    "print \"Data table dimensions 2007-2011: %d x %d\" % data_2007_2011.shape\n",
    "\n",
    "print \"Data table dimensions 2014: %d x %d\" % data_2014.shape\n",
    " \n",
    "print \"Data table dimensions 2015: %d x %d\" % data_2015.shape\n",
    "\n",
    "print \"Data table dimensions 2016: %d x %d\" % data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>emp_title</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>...</th>\n",
       "      <th>num_tl_90g_dpd_24m</th>\n",
       "      <th>num_tl_op_past_12m</th>\n",
       "      <th>pct_tl_nvr_dlq</th>\n",
       "      <th>percent_bc_gt_75</th>\n",
       "      <th>pub_rec_bankruptcies</th>\n",
       "      <th>tax_liens</th>\n",
       "      <th>tot_hi_cred_lim</th>\n",
       "      <th>total_bal_ex_mort</th>\n",
       "      <th>total_bc_limit</th>\n",
       "      <th>total_il_high_credit_limit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th>member_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68516507</th>\n",
       "      <th>73406314</th>\n",
       "      <td>14000</td>\n",
       "      <td>14000</td>\n",
       "      <td>14000</td>\n",
       "      <td>36 months</td>\n",
       "      <td>9.80%</td>\n",
       "      <td>450.43</td>\n",
       "      <td>B</td>\n",
       "      <td>B3</td>\n",
       "      <td>Master mechanic</td>\n",
       "      <td>4 years</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>531602</td>\n",
       "      <td>84039</td>\n",
       "      <td>34500</td>\n",
       "      <td>55842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68587652</th>\n",
       "      <th>73477494</th>\n",
       "      <td>25000</td>\n",
       "      <td>25000</td>\n",
       "      <td>25000</td>\n",
       "      <td>36 months</td>\n",
       "      <td>5.32%</td>\n",
       "      <td>752.87</td>\n",
       "      <td>A</td>\n",
       "      <td>A1</td>\n",
       "      <td>Director</td>\n",
       "      <td>1 year</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>512381</td>\n",
       "      <td>44786</td>\n",
       "      <td>44000</td>\n",
       "      <td>51081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68009401</th>\n",
       "      <th>72868139</th>\n",
       "      <td>16000</td>\n",
       "      <td>16000</td>\n",
       "      <td>16000</td>\n",
       "      <td>60 months</td>\n",
       "      <td>14.85%</td>\n",
       "      <td>379.39</td>\n",
       "      <td>C</td>\n",
       "      <td>C5</td>\n",
       "      <td>Bookkeeper/Accounting</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>78.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>298100</td>\n",
       "      <td>31329</td>\n",
       "      <td>281300</td>\n",
       "      <td>13400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68416935</th>\n",
       "      <th>73306760</th>\n",
       "      <td>15000</td>\n",
       "      <td>15000</td>\n",
       "      <td>15000</td>\n",
       "      <td>36 months</td>\n",
       "      <td>9.80%</td>\n",
       "      <td>482.61</td>\n",
       "      <td>B</td>\n",
       "      <td>B3</td>\n",
       "      <td>Electronic Branch Manager</td>\n",
       "      <td>3 years</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>347470</td>\n",
       "      <td>53397</td>\n",
       "      <td>36100</td>\n",
       "      <td>47370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68357012</th>\n",
       "      <th>73246847</th>\n",
       "      <td>15000</td>\n",
       "      <td>15000</td>\n",
       "      <td>15000</td>\n",
       "      <td>36 months</td>\n",
       "      <td>8.49%</td>\n",
       "      <td>473.45</td>\n",
       "      <td>B</td>\n",
       "      <td>B1</td>\n",
       "      <td>Senior Associate</td>\n",
       "      <td>6 years</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>100.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>369112</td>\n",
       "      <td>27849</td>\n",
       "      <td>28200</td>\n",
       "      <td>21124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    loan_amnt  funded_amnt  funded_amnt_inv        term  \\\n",
       "id       member_id                                                        \n",
       "68516507 73406314       14000        14000            14000   36 months   \n",
       "68587652 73477494       25000        25000            25000   36 months   \n",
       "68009401 72868139       16000        16000            16000   60 months   \n",
       "68416935 73306760       15000        15000            15000   36 months   \n",
       "68357012 73246847       15000        15000            15000   36 months   \n",
       "\n",
       "                   int_rate  installment grade sub_grade  \\\n",
       "id       member_id                                         \n",
       "68516507 73406314     9.80%       450.43     B        B3   \n",
       "68587652 73477494     5.32%       752.87     A        A1   \n",
       "68009401 72868139    14.85%       379.39     C        C5   \n",
       "68416935 73306760     9.80%       482.61     B        B3   \n",
       "68357012 73246847     8.49%       473.45     B        B1   \n",
       "\n",
       "                                    emp_title emp_length  \\\n",
       "id       member_id                                         \n",
       "68516507 73406314             Master mechanic    4 years   \n",
       "68587652 73477494                    Director     1 year   \n",
       "68009401 72868139       Bookkeeper/Accounting  10+ years   \n",
       "68416935 73306760   Electronic Branch Manager    3 years   \n",
       "68357012 73246847            Senior Associate    6 years   \n",
       "\n",
       "                              ...             num_tl_90g_dpd_24m  \\\n",
       "id       member_id            ...                                  \n",
       "68516507 73406314             ...                              0   \n",
       "68587652 73477494             ...                              0   \n",
       "68009401 72868139             ...                              0   \n",
       "68416935 73306760             ...                              0   \n",
       "68357012 73246847             ...                              0   \n",
       "\n",
       "                    num_tl_op_past_12m pct_tl_nvr_dlq percent_bc_gt_75  \\\n",
       "id       member_id                                                       \n",
       "68516507 73406314                    2          100.0              0.0   \n",
       "68587652 73477494                    1          100.0              0.0   \n",
       "68009401 72868139                    2           78.9              0.0   \n",
       "68416935 73306760                    4          100.0              0.0   \n",
       "68357012 73246847                    4          100.0             20.0   \n",
       "\n",
       "                   pub_rec_bankruptcies tax_liens tot_hi_cred_lim  \\\n",
       "id       member_id                                                  \n",
       "68516507 73406314                     0         0          531602   \n",
       "68587652 73477494                     0         0          512381   \n",
       "68009401 72868139                     0         2          298100   \n",
       "68416935 73306760                     1         0          347470   \n",
       "68357012 73246847                     1         0          369112   \n",
       "\n",
       "                   total_bal_ex_mort total_bc_limit total_il_high_credit_limit  \n",
       "id       member_id                                                              \n",
       "68516507 73406314              84039          34500                      55842  \n",
       "68587652 73477494              44786          44000                      51081  \n",
       "68009401 72868139              31329         281300                      13400  \n",
       "68416935 73306760              53397          36100                      47370  \n",
       "68357012 73246847              27849          28200                      21124  \n",
       "\n",
       "[5 rows x 109 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>emp_title</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>...</th>\n",
       "      <th>num_tl_90g_dpd_24m</th>\n",
       "      <th>num_tl_op_past_12m</th>\n",
       "      <th>pct_tl_nvr_dlq</th>\n",
       "      <th>percent_bc_gt_75</th>\n",
       "      <th>pub_rec_bankruptcies</th>\n",
       "      <th>tax_liens</th>\n",
       "      <th>tot_hi_cred_lim</th>\n",
       "      <th>total_bal_ex_mort</th>\n",
       "      <th>total_bc_limit</th>\n",
       "      <th>total_il_high_credit_limit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th>member_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38098114</th>\n",
       "      <th>40860827</th>\n",
       "      <td>15000</td>\n",
       "      <td>15000</td>\n",
       "      <td>15000</td>\n",
       "      <td>60 months</td>\n",
       "      <td>12.39%</td>\n",
       "      <td>336.64</td>\n",
       "      <td>C</td>\n",
       "      <td>C1</td>\n",
       "      <td>MANAGEMENT</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>196500</td>\n",
       "      <td>149140</td>\n",
       "      <td>10000</td>\n",
       "      <td>12000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36805548</th>\n",
       "      <th>39558264</th>\n",
       "      <td>10400</td>\n",
       "      <td>10400</td>\n",
       "      <td>10400</td>\n",
       "      <td>36 months</td>\n",
       "      <td>6.99%</td>\n",
       "      <td>321.08</td>\n",
       "      <td>A</td>\n",
       "      <td>A3</td>\n",
       "      <td>Truck Driver Delivery Personel</td>\n",
       "      <td>8 years</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>83.3</td>\n",
       "      <td>14.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>179407</td>\n",
       "      <td>15030</td>\n",
       "      <td>13000</td>\n",
       "      <td>11325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37822187</th>\n",
       "      <th>40585251</th>\n",
       "      <td>9600</td>\n",
       "      <td>9600</td>\n",
       "      <td>9600</td>\n",
       "      <td>36 months</td>\n",
       "      <td>13.66%</td>\n",
       "      <td>326.53</td>\n",
       "      <td>C</td>\n",
       "      <td>C3</td>\n",
       "      <td>Admin Specialist</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>100.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52490</td>\n",
       "      <td>38566</td>\n",
       "      <td>21100</td>\n",
       "      <td>24890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37612354</th>\n",
       "      <th>40375473</th>\n",
       "      <td>12800</td>\n",
       "      <td>12800</td>\n",
       "      <td>12800</td>\n",
       "      <td>60 months</td>\n",
       "      <td>17.14%</td>\n",
       "      <td>319.08</td>\n",
       "      <td>D</td>\n",
       "      <td>D4</td>\n",
       "      <td>Senior Sales Professional</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>76.9</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>368700</td>\n",
       "      <td>18007</td>\n",
       "      <td>4400</td>\n",
       "      <td>18000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37662224</th>\n",
       "      <th>40425321</th>\n",
       "      <td>7650</td>\n",
       "      <td>7650</td>\n",
       "      <td>7650</td>\n",
       "      <td>36 months</td>\n",
       "      <td>13.66%</td>\n",
       "      <td>260.20</td>\n",
       "      <td>C</td>\n",
       "      <td>C3</td>\n",
       "      <td>Technical Specialist</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>82331</td>\n",
       "      <td>64426</td>\n",
       "      <td>4900</td>\n",
       "      <td>64031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    loan_amnt  funded_amnt  funded_amnt_inv        term  \\\n",
       "id       member_id                                                        \n",
       "38098114 40860827       15000        15000            15000   60 months   \n",
       "36805548 39558264       10400        10400            10400   36 months   \n",
       "37822187 40585251        9600         9600             9600   36 months   \n",
       "37612354 40375473       12800        12800            12800   60 months   \n",
       "37662224 40425321        7650         7650             7650   36 months   \n",
       "\n",
       "                   int_rate  installment grade sub_grade  \\\n",
       "id       member_id                                         \n",
       "38098114 40860827    12.39%       336.64     C        C1   \n",
       "36805548 39558264     6.99%       321.08     A        A3   \n",
       "37822187 40585251    13.66%       326.53     C        C3   \n",
       "37612354 40375473    17.14%       319.08     D        D4   \n",
       "37662224 40425321    13.66%       260.20     C        C3   \n",
       "\n",
       "                                         emp_title emp_length  \\\n",
       "id       member_id                                              \n",
       "38098114 40860827                       MANAGEMENT  10+ years   \n",
       "36805548 39558264   Truck Driver Delivery Personel    8 years   \n",
       "37822187 40585251                 Admin Specialist  10+ years   \n",
       "37612354 40375473        Senior Sales Professional  10+ years   \n",
       "37662224 40425321             Technical Specialist   < 1 year   \n",
       "\n",
       "                              ...             num_tl_90g_dpd_24m  \\\n",
       "id       member_id            ...                                  \n",
       "38098114 40860827             ...                              0   \n",
       "36805548 39558264             ...                              0   \n",
       "37822187 40585251             ...                              0   \n",
       "37612354 40375473             ...                              0   \n",
       "37662224 40425321             ...                              0   \n",
       "\n",
       "                    num_tl_op_past_12m pct_tl_nvr_dlq percent_bc_gt_75  \\\n",
       "id       member_id                                                       \n",
       "38098114 40860827                    4          100.0              0.0   \n",
       "36805548 39558264                    4           83.3             14.3   \n",
       "37822187 40585251                    3          100.0             60.0   \n",
       "37612354 40375473                    0           76.9            100.0   \n",
       "37662224 40425321                    2          100.0            100.0   \n",
       "\n",
       "                   pub_rec_bankruptcies tax_liens tot_hi_cred_lim  \\\n",
       "id       member_id                                                  \n",
       "38098114 40860827                     0         0          196500   \n",
       "36805548 39558264                     0         0          179407   \n",
       "37822187 40585251                     0         0           52490   \n",
       "37612354 40375473                     0         0          368700   \n",
       "37662224 40425321                     0         0           82331   \n",
       "\n",
       "                   total_bal_ex_mort total_bc_limit total_il_high_credit_limit  \n",
       "id       member_id                                                              \n",
       "38098114 40860827             149140          10000                      12000  \n",
       "36805548 39558264              15030          13000                      11325  \n",
       "37822187 40585251              38566          21100                      24890  \n",
       "37612354 40375473              18007           4400                      18000  \n",
       "37662224 40425321              64426           4900                      64031  \n",
       "\n",
       "[5 rows x 109 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2015.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the count of fields by data type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>emp_title</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>...</th>\n",
       "      <th>num_tl_90g_dpd_24m</th>\n",
       "      <th>num_tl_op_past_12m</th>\n",
       "      <th>pct_tl_nvr_dlq</th>\n",
       "      <th>percent_bc_gt_75</th>\n",
       "      <th>pub_rec_bankruptcies</th>\n",
       "      <th>tax_liens</th>\n",
       "      <th>tot_hi_cred_lim</th>\n",
       "      <th>total_bal_ex_mort</th>\n",
       "      <th>total_bc_limit</th>\n",
       "      <th>total_il_high_credit_limit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th>member_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10159611</th>\n",
       "      <th>12011228</th>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>9.67%</td>\n",
       "      <td>321.13</td>\n",
       "      <td>B</td>\n",
       "      <td>B1</td>\n",
       "      <td>Registered Nurse</td>\n",
       "      <td>7 years</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>77.3</td>\n",
       "      <td>66.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58486.0</td>\n",
       "      <td>39143.0</td>\n",
       "      <td>9200.0</td>\n",
       "      <td>36186.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10129477</th>\n",
       "      <th>11981093</th>\n",
       "      <td>14000</td>\n",
       "      <td>14000</td>\n",
       "      <td>14000.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>12.85%</td>\n",
       "      <td>470.71</td>\n",
       "      <td>B</td>\n",
       "      <td>B4</td>\n",
       "      <td>Assistant Director - Human Resources</td>\n",
       "      <td>4 years</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.6</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>31840.0</td>\n",
       "      <td>17672.0</td>\n",
       "      <td>3900.0</td>\n",
       "      <td>27340.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10149342</th>\n",
       "      <th>12000897</th>\n",
       "      <td>27050</td>\n",
       "      <td>27050</td>\n",
       "      <td>27050.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>10.99%</td>\n",
       "      <td>885.46</td>\n",
       "      <td>B</td>\n",
       "      <td>B2</td>\n",
       "      <td>Team Leadern Customer Ops &amp; Systems</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138554.0</td>\n",
       "      <td>70186.0</td>\n",
       "      <td>35700.0</td>\n",
       "      <td>33054.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10139658</th>\n",
       "      <th>11991209</th>\n",
       "      <td>12000</td>\n",
       "      <td>12000</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>13.53%</td>\n",
       "      <td>407.40</td>\n",
       "      <td>B</td>\n",
       "      <td>B5</td>\n",
       "      <td>On road manager</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>81.2</td>\n",
       "      <td>33.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18130.0</td>\n",
       "      <td>13605.0</td>\n",
       "      <td>7000.0</td>\n",
       "      <td>10030.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10159548</th>\n",
       "      <th>12011167</th>\n",
       "      <td>15000</td>\n",
       "      <td>15000</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>8.90%</td>\n",
       "      <td>476.30</td>\n",
       "      <td>A</td>\n",
       "      <td>A5</td>\n",
       "      <td>aircraft maintenance engineer</td>\n",
       "      <td>2 years</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.3</td>\n",
       "      <td>66.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>288195.0</td>\n",
       "      <td>39448.0</td>\n",
       "      <td>14200.0</td>\n",
       "      <td>33895.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    loan_amnt  funded_amnt  funded_amnt_inv        term  \\\n",
       "id       member_id                                                        \n",
       "10159611 12011228       10000        10000          10000.0   36 months   \n",
       "10129477 11981093       14000        14000          14000.0   36 months   \n",
       "10149342 12000897       27050        27050          27050.0   36 months   \n",
       "10139658 11991209       12000        12000          12000.0   36 months   \n",
       "10159548 12011167       15000        15000          15000.0   36 months   \n",
       "\n",
       "                   int_rate  installment grade sub_grade  \\\n",
       "id       member_id                                         \n",
       "10159611 12011228     9.67%       321.13     B        B1   \n",
       "10129477 11981093    12.85%       470.71     B        B4   \n",
       "10149342 12000897    10.99%       885.46     B        B2   \n",
       "10139658 11991209    13.53%       407.40     B        B5   \n",
       "10159548 12011167     8.90%       476.30     A        A5   \n",
       "\n",
       "                                               emp_title emp_length  \\\n",
       "id       member_id                                                    \n",
       "10159611 12011228                       Registered Nurse    7 years   \n",
       "10129477 11981093   Assistant Director - Human Resources    4 years   \n",
       "10149342 12000897    Team Leadern Customer Ops & Systems  10+ years   \n",
       "10139658 11991209                        On road manager  10+ years   \n",
       "10159548 12011167          aircraft maintenance engineer    2 years   \n",
       "\n",
       "                              ...             num_tl_90g_dpd_24m  \\\n",
       "id       member_id            ...                                  \n",
       "10159611 12011228             ...                            0.0   \n",
       "10129477 11981093             ...                            0.0   \n",
       "10149342 12000897             ...                            0.0   \n",
       "10139658 11991209             ...                            0.0   \n",
       "10159548 12011167             ...                            0.0   \n",
       "\n",
       "                    num_tl_op_past_12m pct_tl_nvr_dlq percent_bc_gt_75  \\\n",
       "id       member_id                                                       \n",
       "10159611 12011228                  1.0           77.3             66.7   \n",
       "10129477 11981093                  0.0           78.6            100.0   \n",
       "10149342 12000897                  1.0          100.0             25.0   \n",
       "10139658 11991209                  2.0           81.2             33.3   \n",
       "10159548 12011167                  0.0           89.3             66.7   \n",
       "\n",
       "                   pub_rec_bankruptcies tax_liens tot_hi_cred_lim  \\\n",
       "id       member_id                                                  \n",
       "10159611 12011228                     0         0         58486.0   \n",
       "10129477 11981093                     1         0         31840.0   \n",
       "10149342 12000897                     0         0        138554.0   \n",
       "10139658 11991209                     0         0         18130.0   \n",
       "10159548 12011167                     0         0        288195.0   \n",
       "\n",
       "                   total_bal_ex_mort total_bc_limit total_il_high_credit_limit  \n",
       "id       member_id                                                              \n",
       "10159611 12011228            39143.0         9200.0                    36186.0  \n",
       "10129477 11981093            17672.0         3900.0                    27340.0  \n",
       "10149342 12000897            70186.0        35700.0                    33054.0  \n",
       "10139658 11991209            13605.0         7000.0                    10030.0  \n",
       "10159548 12011167            39448.0        14200.0                    33895.0  \n",
       "\n",
       "[5 rows x 109 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2014.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>emp_title</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>...</th>\n",
       "      <th>num_tl_90g_dpd_24m</th>\n",
       "      <th>num_tl_op_past_12m</th>\n",
       "      <th>pct_tl_nvr_dlq</th>\n",
       "      <th>percent_bc_gt_75</th>\n",
       "      <th>pub_rec_bankruptcies</th>\n",
       "      <th>tax_liens</th>\n",
       "      <th>tot_hi_cred_lim</th>\n",
       "      <th>total_bal_ex_mort</th>\n",
       "      <th>total_bc_limit</th>\n",
       "      <th>total_il_high_credit_limit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th>member_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1077501</th>\n",
       "      <th>1296599.0</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>4975.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>10.65%</td>\n",
       "      <td>162.87</td>\n",
       "      <td>B</td>\n",
       "      <td>B2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077430</th>\n",
       "      <th>1314167.0</th>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>15.27%</td>\n",
       "      <td>59.83</td>\n",
       "      <td>C</td>\n",
       "      <td>C4</td>\n",
       "      <td>Ryder</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077175</th>\n",
       "      <th>1313524.0</th>\n",
       "      <td>2400.0</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>15.96%</td>\n",
       "      <td>84.33</td>\n",
       "      <td>C</td>\n",
       "      <td>C5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076863</th>\n",
       "      <th>1277178.0</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>13.49%</td>\n",
       "      <td>339.31</td>\n",
       "      <td>C</td>\n",
       "      <td>C1</td>\n",
       "      <td>AIR RESOURCES BOARD</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075358</th>\n",
       "      <th>1311748.0</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>12.69%</td>\n",
       "      <td>67.79</td>\n",
       "      <td>B</td>\n",
       "      <td>B5</td>\n",
       "      <td>University Medical Group</td>\n",
       "      <td>1 year</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   loan_amnt  funded_amnt  funded_amnt_inv        term  \\\n",
       "id      member_id                                                        \n",
       "1077501 1296599.0     5000.0       5000.0           4975.0   36 months   \n",
       "1077430 1314167.0     2500.0       2500.0           2500.0   60 months   \n",
       "1077175 1313524.0     2400.0       2400.0           2400.0   36 months   \n",
       "1076863 1277178.0    10000.0      10000.0          10000.0   36 months   \n",
       "1075358 1311748.0     3000.0       3000.0           3000.0   60 months   \n",
       "\n",
       "                  int_rate  installment grade sub_grade  \\\n",
       "id      member_id                                         \n",
       "1077501 1296599.0   10.65%       162.87     B        B2   \n",
       "1077430 1314167.0   15.27%        59.83     C        C4   \n",
       "1077175 1313524.0   15.96%        84.33     C        C5   \n",
       "1076863 1277178.0   13.49%       339.31     C        C1   \n",
       "1075358 1311748.0   12.69%        67.79     B        B5   \n",
       "\n",
       "                                  emp_title emp_length  \\\n",
       "id      member_id                                        \n",
       "1077501 1296599.0                       NaN  10+ years   \n",
       "1077430 1314167.0                     Ryder   < 1 year   \n",
       "1077175 1313524.0                       NaN  10+ years   \n",
       "1076863 1277178.0       AIR RESOURCES BOARD  10+ years   \n",
       "1075358 1311748.0  University Medical Group     1 year   \n",
       "\n",
       "                             ...             num_tl_90g_dpd_24m  \\\n",
       "id      member_id            ...                                  \n",
       "1077501 1296599.0            ...                            NaN   \n",
       "1077430 1314167.0            ...                            NaN   \n",
       "1077175 1313524.0            ...                            NaN   \n",
       "1076863 1277178.0            ...                            NaN   \n",
       "1075358 1311748.0            ...                            NaN   \n",
       "\n",
       "                   num_tl_op_past_12m pct_tl_nvr_dlq percent_bc_gt_75  \\\n",
       "id      member_id                                                       \n",
       "1077501 1296599.0                 NaN            NaN              NaN   \n",
       "1077430 1314167.0                 NaN            NaN              NaN   \n",
       "1077175 1313524.0                 NaN            NaN              NaN   \n",
       "1076863 1277178.0                 NaN            NaN              NaN   \n",
       "1075358 1311748.0                 NaN            NaN              NaN   \n",
       "\n",
       "                  pub_rec_bankruptcies tax_liens tot_hi_cred_lim  \\\n",
       "id      member_id                                                  \n",
       "1077501 1296599.0                  0.0       0.0             NaN   \n",
       "1077430 1314167.0                  0.0       0.0             NaN   \n",
       "1077175 1313524.0                  0.0       0.0             NaN   \n",
       "1076863 1277178.0                  0.0       0.0             NaN   \n",
       "1075358 1311748.0                  0.0       0.0             NaN   \n",
       "\n",
       "                  total_bal_ex_mort total_bc_limit total_il_high_credit_limit  \n",
       "id      member_id                                                              \n",
       "1077501 1296599.0               NaN            NaN                        NaN  \n",
       "1077430 1314167.0               NaN            NaN                        NaN  \n",
       "1077175 1313524.0               NaN            NaN                        NaN  \n",
       "1076863 1277178.0               NaN            NaN                        NaN  \n",
       "1075358 1311748.0               NaN            NaN                        NaN  \n",
       "\n",
       "[5 rows x 109 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2007_2011.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64    83\n",
      "object     26\n",
      "dtype: int64\n",
      "\n",
      "float64    69\n",
      "object     26\n",
      "int64      14\n",
      "dtype: int64\n",
      "\n",
      "float64    43\n",
      "int64      40\n",
      "object     26\n",
      "dtype: int64\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "float64    43\n",
       "int64      40\n",
       "object     26\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print data_2007_2011.dtypes.value_counts()\n",
    "print\n",
    "print data_2014.dtypes.value_counts()\n",
    "print\n",
    "print data_2015.dtypes.value_counts()\n",
    "print\n",
    "data.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the specific dataset used, the numeric values may be read in as integers. For best performance and to enable mergining of the datasets, we convert those fields to floats (which allow NaN values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k, v in convert_float.items():\n",
    "    data_2007_2011[k] = data_2007_2011[k].astype(v)\n",
    "    data_2014[k] = data_2014[k].astype(v)\n",
    "    data_2015[k] = data_2015[k].astype(v)\n",
    "    data[k] = data[k].astype(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the data types after the float conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64    83\n",
      "object     26\n",
      "dtype: int64\n",
      "\n",
      "float64    83\n",
      "object     26\n",
      "dtype: int64\n",
      "\n",
      "float64    83\n",
      "object     26\n",
      "dtype: int64\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "float64    83\n",
       "object     26\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print data_2007_2011.dtypes.value_counts()\n",
    "print\n",
    "print data_2014.dtypes.value_counts()\n",
    "print\n",
    "print data_2015.dtypes.value_counts()\n",
    "print\n",
    "data.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object fields need some more processing. First, we list out those fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "term                         object\n",
       "int_rate                     object\n",
       "grade                        object\n",
       "sub_grade                    object\n",
       "emp_title                    object\n",
       "emp_length                   object\n",
       "home_ownership               object\n",
       "verification_status          object\n",
       "issue_d                      object\n",
       "loan_status                  object\n",
       "pymnt_plan                   object\n",
       "url                          object\n",
       "desc                         object\n",
       "purpose                      object\n",
       "title                        object\n",
       "zip_code                     object\n",
       "addr_state                   object\n",
       "earliest_cr_line             object\n",
       "revol_util                   object\n",
       "initial_list_status          object\n",
       "last_pymnt_d                 object\n",
       "next_pymnt_d                 object\n",
       "last_credit_pull_d           object\n",
       "policy_code                  object\n",
       "application_type             object\n",
       "verification_status_joint    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes[data.dtypes==\"object\"]\n",
    "data_2007_2011.dtypes[data_2007_2011.dtypes==\"object\"]\n",
    "data_2014.dtypes[data_2014.dtypes==\"object\"]\n",
    "data_2015.dtypes[data_2015.dtypes==\"object\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 5 object fields that contain dates in the format *YYYY-MMM* (e.g. '2010-Jan'). We parse those to return datetime fields, which are more easily input into time series models or plotted in charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.issue_d = pd.to_datetime(data.issue_d, errors=\"coerce\")\n",
    "data.last_pymnt_d = pd.to_datetime(data.last_pymnt_d, errors=\"coerce\")\n",
    "data.next_pymnt_d = pd.to_datetime(data.next_pymnt_d, errors=\"coerce\")\n",
    "data.last_credit_pull_d = pd.to_datetime(data.last_credit_pull_d, errors=\"coerce\")\n",
    "data.earliest_cr_line = pd.to_datetime(data.earliest_cr_line, errors=\"coerce\")\n",
    "\n",
    "data_2007_2011.issue_d = pd.to_datetime(data_2007_2011.issue_d, errors=\"coerce\")\n",
    "data_2007_2011.last_pymnt_d = pd.to_datetime(data_2007_2011.last_pymnt_d, errors=\"coerce\")\n",
    "data_2007_2011.next_pymnt_d = pd.to_datetime(data_2007_2011.next_pymnt_d, errors=\"coerce\")\n",
    "data_2007_2011.last_credit_pull_d = pd.to_datetime(data_2007_2011.last_credit_pull_d, errors=\"coerce\")\n",
    "data_2007_2011.earliest_cr_line = pd.to_datetime(data_2007_2011.earliest_cr_line, errors=\"coerce\")\n",
    "\n",
    "data_2014.issue_d = pd.to_datetime(data_2014.issue_d, errors=\"coerce\")\n",
    "data_2014.last_pymnt_d = pd.to_datetime(data_2014.last_pymnt_d, errors=\"coerce\")\n",
    "data_2014.next_pymnt_d = pd.to_datetime(data_2014.next_pymnt_d, errors=\"coerce\")\n",
    "data_2014.last_credit_pull_d = pd.to_datetime(data_2014.last_credit_pull_d, errors=\"coerce\")\n",
    "data_2014.earliest_cr_line = pd.to_datetime(data_2014.earliest_cr_line, errors=\"coerce\")\n",
    "\n",
    "data_2015.issue_d = pd.to_datetime(data_2015.issue_d, errors=\"coerce\")\n",
    "data_2015.last_pymnt_d = pd.to_datetime(data_2015.last_pymnt_d, errors=\"coerce\")\n",
    "data_2015.next_pymnt_d = pd.to_datetime(data_2015.next_pymnt_d, errors=\"coerce\")\n",
    "data_2015.last_credit_pull_d = pd.to_datetime(data_2015.last_credit_pull_d, errors=\"coerce\")\n",
    "data_2015.earliest_cr_line = pd.to_datetime(data_2015.earliest_cr_line, errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the remaining fields contain categorical data. We use the pandas *category* data type to store the data more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.term = pd.Categorical(data.term, categories= [\" 36 months\", \" 60 months\", \"None\"])\n",
    "data.grade = pd.Categorical(data.grade, categories=[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"None\"])\n",
    "data.sub_grade = pd.Categorical(data.sub_grade, categories=[\"A1\", \"A2\", \"A3\", \"A4\", \"A5\",\n",
    "                                                            \"B1\", \"B2\", \"B3\", \"B4\", \"B5\", \n",
    "                                                            \"C1\", \"C2\", \"C3\", \"C4\", \"C5\",\n",
    "                                                            \"D1\", \"D2\", \"D3\", \"D4\", \"D5\",\n",
    "                                                            \"E1\", \"E2\", \"E3\", \"E4\", \"E5\",\n",
    "                                                            \"F1\", \"F2\", \"F3\", \"F4\", \"F5\",\n",
    "                                                            \"G1\", \"G2\", \"G3\", \"G4\", \"G5\",\n",
    "                                                            \"None\"])\n",
    "data.home_ownership = pd.Categorical(data.home_ownership.str.title(), categories=[\"Own\", \"Mortgage\", \"Rent\", \"Any\", \"Other\", \"None\"])\n",
    "data.emp_length = pd.Categorical(data.emp_length, categories=[\"< 1 year\", \"1 year\", \"2 years\", \"3 years\",\n",
    "                                                              \"4 years\", \"5 years\", \"6 years\", \"7 years\",\n",
    "                                                              \"8 years\", \"9 years\", \"10+ years\", \"n/a\", \"None\"])\n",
    "data.verification_status = pd.Categorical(data.verification_status, categories=[\"Verified\", \"Source Verified\", \"Not Verified\", \"None\"])\n",
    "data.loan_status = pd.Categorical(data.loan_status, categories=[\"Fully Paid\", \"Current\", \"Charged Off\",\n",
    "                                                                \"Does not meet the credit policy. Status:Fully Paid\",\n",
    "                                                                \"Does not meet the credit policy. Status:Charged Off\",\n",
    "                                                                \"In Grace Period\", \"Late (16-30 days)\", \"Late (31-120 days)\",\n",
    "                                                                \"Default\", \"None\"])\n",
    "data.pymnt_plan = pd.Categorical(data.pymnt_plan.str.title(), categories=[\"Y\", \"N\", \"None\"])\n",
    "data.purpose = pd.Categorical(data.purpose.str.title(),\n",
    "                              categories=[\"Debt_Consolidation\", \"Credit_Card\", \"Home_Improvement\", \"Major_Purchase\", \n",
    "                                          \"Small_Business\", \"Car\", \"Wedding\", \"Medical\", \"Moving\", \"House\",\n",
    "                                          \"Educational\", \"Vacation\", \"Renewable_Energy\", \"Other\", \"None\"])\n",
    "data.addr_state = pd.Categorical(data.addr_state,\n",
    "                                 categories=[\"AK\", \"AL\", \"AR\", \"AZ\", \"CA\", \"CO\", \"CT\", \"DC\", \"DE\", \"FL\",\n",
    "                                             \"GA\", \"HI\", \"IA\", \"ID\", \"IL\", \"IN\", \"KS\", \"KY\", \"LA\", \"MA\",\n",
    "                                             \"MD\", \"ME\", \"MI\", \"MN\", \"MO\", \"MS\", \"MT\", \"NC\", \"ND\", \"NE\",\n",
    "                                             \"NH\", \"NJ\", \"NM\", \"NV\", \"NY\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\",\n",
    "                                             \"SC\", \"SD\", \"TN\", \"TX\", \"UT\", \"VA\", \"VT\", \"WA\", \"WI\", \"WV\",\n",
    "                                             \"WY\", \"None\"])\n",
    "data.initial_list_status = pd.Categorical(data.initial_list_status, categories=[\"f\", \"w\", \"None\"])\n",
    "data.policy_code = pd.Categorical(data.policy_code, categories=[\"1\", \"None\"])\n",
    "data.application_type = pd.Categorical(data.application_type.str.title(), categories=[\"Individual\", \"Joint\", \"None\"])\n",
    "data.verification_status_joint = pd.Categorical(data.verification_status_joint, categories=[\"Verified\", \"Source Verified\", \"Not Verified\", \"None\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_2007_2011.term = pd.Categorical(data_2007_2011.term, categories= [\" 36 months\", \" 60 months\", \"None\"])\n",
    "data_2007_2011.grade = pd.Categorical(data_2007_2011.grade, categories=[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"None\"])\n",
    "data_2007_2011.sub_grade = pd.Categorical(data_2007_2011.sub_grade, categories=[\"A1\", \"A2\", \"A3\", \"A4\", \"A5\",\n",
    "                                                            \"B1\", \"B2\", \"B3\", \"B4\", \"B5\", \n",
    "                                                            \"C1\", \"C2\", \"C3\", \"C4\", \"C5\",\n",
    "                                                            \"D1\", \"D2\", \"D3\", \"D4\", \"D5\",\n",
    "                                                            \"E1\", \"E2\", \"E3\", \"E4\", \"E5\",\n",
    "                                                            \"F1\", \"F2\", \"F3\", \"F4\", \"F5\",\n",
    "                                                            \"G1\", \"G2\", \"G3\", \"G4\", \"G5\",\n",
    "                                                            \"None\"])\n",
    "data_2007_2011.home_ownership = pd.Categorical(data_2007_2011.home_ownership.str.title(), categories=[\"Own\", \"Mortgage\", \"Rent\", \"Any\", \"Other\", \"None\"])\n",
    "data_2007_2011.emp_length = pd.Categorical(data_2007_2011.emp_length, categories=[\"< 1 year\", \"1 year\", \"2 years\", \"3 years\",\n",
    "                                                              \"4 years\", \"5 years\", \"6 years\", \"7 years\",\n",
    "                                                              \"8 years\", \"9 years\", \"10+ years\", \"n/a\", \"None\"])\n",
    "data_2007_2011.verification_status = pd.Categorical(data_2007_2011.verification_status, categories=[\"Verified\", \"Source Verified\", \"Not Verified\", \"None\"])\n",
    "data_2007_2011.loan_status = pd.Categorical(data_2007_2011.loan_status, categories=[\"Fully Paid\", \"Current\", \"Charged Off\",\n",
    "                                                                \"Does not meet the credit policy. Status:Fully Paid\",\n",
    "                                                                \"Does not meet the credit policy. Status:Charged Off\",\n",
    "                                                                \"In Grace Period\", \"Late (16-30 days)\", \"Late (31-120 days)\",\n",
    "                                                                \"Default\", \"None\"])\n",
    "data_2007_2011.pymnt_plan = pd.Categorical(data_2007_2011.pymnt_plan.str.title(), categories=[\"Y\", \"N\", \"None\"])\n",
    "data_2007_2011.purpose = pd.Categorical(data_2007_2011.purpose.str.title(),\n",
    "                              categories=[\"Debt_Consolidation\", \"Credit_Card\", \"Home_Improvement\", \"Major_Purchase\", \n",
    "                                          \"Small_Business\", \"Car\", \"Wedding\", \"Medical\", \"Moving\", \"House\",\n",
    "                                          \"Educational\", \"Vacation\", \"Renewable_Energy\", \"Other\", \"None\"])\n",
    "data_2007_2011.addr_state = pd.Categorical(data_2007_2011.addr_state,\n",
    "                                 categories=[\"AK\", \"AL\", \"AR\", \"AZ\", \"CA\", \"CO\", \"CT\", \"DC\", \"DE\", \"FL\",\n",
    "                                             \"GA\", \"HI\", \"IA\", \"ID\", \"IL\", \"IN\", \"KS\", \"KY\", \"LA\", \"MA\",\n",
    "                                             \"MD\", \"ME\", \"MI\", \"MN\", \"MO\", \"MS\", \"MT\", \"NC\", \"ND\", \"NE\",\n",
    "                                             \"NH\", \"NJ\", \"NM\", \"NV\", \"NY\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\",\n",
    "                                             \"SC\", \"SD\", \"TN\", \"TX\", \"UT\", \"VA\", \"VT\", \"WA\", \"WI\", \"WV\",\n",
    "                                             \"WY\", \"None\"])\n",
    "data_2007_2011.initial_list_status = pd.Categorical(data_2007_2011.initial_list_status, categories=[\"f\", \"w\", \"None\"])\n",
    "data_2007_2011.policy_code = pd.Categorical(data_2007_2011.policy_code, categories=[\"1\", \"None\"])\n",
    "data_2007_2011.application_type = pd.Categorical(data_2007_2011.application_type.str.title(), categories=[\"Individual\", \"Joint\", \"None\"])\n",
    "data_2007_2011.verification_status_joint = pd.Categorical(data_2007_2011.verification_status_joint, categories=[\"Verified\", \"Source Verified\", \"Not Verified\", \"None\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_2014.term = pd.Categorical(data_2014.term, categories= [\" 36 months\", \" 60 months\", \"None\"])\n",
    "data_2014.grade = pd.Categorical(data_2014.grade, categories=[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"None\"])\n",
    "data_2014.sub_grade = pd.Categorical(data_2014.sub_grade, categories=[\"A1\", \"A2\", \"A3\", \"A4\", \"A5\",\n",
    "                                                            \"B1\", \"B2\", \"B3\", \"B4\", \"B5\", \n",
    "                                                            \"C1\", \"C2\", \"C3\", \"C4\", \"C5\",\n",
    "                                                            \"D1\", \"D2\", \"D3\", \"D4\", \"D5\",\n",
    "                                                            \"E1\", \"E2\", \"E3\", \"E4\", \"E5\",\n",
    "                                                            \"F1\", \"F2\", \"F3\", \"F4\", \"F5\",\n",
    "                                                            \"G1\", \"G2\", \"G3\", \"G4\", \"G5\",\n",
    "                                                            \"None\"])\n",
    "data_2014.home_ownership = pd.Categorical(data_2014.home_ownership.str.title(), categories=[\"Own\", \"Mortgage\", \"Rent\", \"Any\", \"Other\", \"None\"])\n",
    "data_2014.emp_length = pd.Categorical(data_2014.emp_length, categories=[\"< 1 year\", \"1 year\", \"2 years\", \"3 years\",\n",
    "                                                              \"4 years\", \"5 years\", \"6 years\", \"7 years\",\n",
    "                                                              \"8 years\", \"9 years\", \"10+ years\", \"n/a\", \"None\"])\n",
    "data_2014.verification_status = pd.Categorical(data_2014.verification_status, categories=[\"Verified\", \"Source Verified\", \"Not Verified\", \"None\"])\n",
    "data_2014.loan_status = pd.Categorical(data_2014.loan_status, categories=[\"Fully Paid\", \"Current\", \"Charged Off\",\n",
    "                                                                \"Does not meet the credit policy. Status:Fully Paid\",\n",
    "                                                                \"Does not meet the credit policy. Status:Charged Off\",\n",
    "                                                                \"In Grace Period\", \"Late (16-30 days)\", \"Late (31-120 days)\",\n",
    "                                                                \"Default\", \"None\"])\n",
    "data_2014.pymnt_plan = pd.Categorical(data_2014.pymnt_plan.str.title(), categories=[\"Y\", \"N\", \"None\"])\n",
    "data_2014.purpose = pd.Categorical(data_2014.purpose.str.title(),\n",
    "                              categories=[\"Debt_Consolidation\", \"Credit_Card\", \"Home_Improvement\", \"Major_Purchase\", \n",
    "                                          \"Small_Business\", \"Car\", \"Wedding\", \"Medical\", \"Moving\", \"House\",\n",
    "                                          \"Educational\", \"Vacation\", \"Renewable_Energy\", \"Other\", \"None\"])\n",
    "data_2014.addr_state = pd.Categorical(data_2014.addr_state,\n",
    "                                 categories=[\"AK\", \"AL\", \"AR\", \"AZ\", \"CA\", \"CO\", \"CT\", \"DC\", \"DE\", \"FL\",\n",
    "                                             \"GA\", \"HI\", \"IA\", \"ID\", \"IL\", \"IN\", \"KS\", \"KY\", \"LA\", \"MA\",\n",
    "                                             \"MD\", \"ME\", \"MI\", \"MN\", \"MO\", \"MS\", \"MT\", \"NC\", \"ND\", \"NE\",\n",
    "                                             \"NH\", \"NJ\", \"NM\", \"NV\", \"NY\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\",\n",
    "                                             \"SC\", \"SD\", \"TN\", \"TX\", \"UT\", \"VA\", \"VT\", \"WA\", \"WI\", \"WV\",\n",
    "                                             \"WY\", \"None\"])\n",
    "data_2014.initial_list_status = pd.Categorical(data_2014.initial_list_status, categories=[\"f\", \"w\", \"None\"])\n",
    "data_2014.policy_code = pd.Categorical(data_2014.policy_code, categories=[\"1\", \"None\"])\n",
    "data_2014.application_type = pd.Categorical(data_2014.application_type.str.title(), categories=[\"Individual\", \"Joint\", \"None\"])\n",
    "data_2014.verification_status_joint = pd.Categorical(data_2014.verification_status_joint, categories=[\"Verified\", \"Source Verified\", \"Not Verified\", \"None\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_2015.term = pd.Categorical(data_2015.term, categories= [\" 36 months\", \" 60 months\", \"None\"])\n",
    "data_2015.grade = pd.Categorical(data_2015.grade, categories=[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"None\"])\n",
    "data_2015.sub_grade = pd.Categorical(data_2015.sub_grade, categories=[\"A1\", \"A2\", \"A3\", \"A4\", \"A5\",\n",
    "                                                            \"B1\", \"B2\", \"B3\", \"B4\", \"B5\", \n",
    "                                                            \"C1\", \"C2\", \"C3\", \"C4\", \"C5\",\n",
    "                                                            \"D1\", \"D2\", \"D3\", \"D4\", \"D5\",\n",
    "                                                            \"E1\", \"E2\", \"E3\", \"E4\", \"E5\",\n",
    "                                                            \"F1\", \"F2\", \"F3\", \"F4\", \"F5\",\n",
    "                                                            \"G1\", \"G2\", \"G3\", \"G4\", \"G5\",\n",
    "                                                            \"None\"])\n",
    "data_2015.home_ownership = pd.Categorical(data_2015.home_ownership.str.title(), categories=[\"Own\", \"Mortgage\", \"Rent\", \"Any\", \"Other\", \"None\"])\n",
    "data_2015.emp_length = pd.Categorical(data_2015.emp_length, categories=[\"< 1 year\", \"1 year\", \"2 years\", \"3 years\",\n",
    "                                                              \"4 years\", \"5 years\", \"6 years\", \"7 years\",\n",
    "                                                              \"8 years\", \"9 years\", \"10+ years\", \"n/a\", \"None\"])\n",
    "data_2015.verification_status = pd.Categorical(data_2015.verification_status, categories=[\"Verified\", \"Source Verified\", \"Not Verified\", \"None\"])\n",
    "data_2015.loan_status = pd.Categorical(data_2015.loan_status, categories=[\"Fully Paid\", \"Current\", \"Charged Off\",\n",
    "                                                                \"Does not meet the credit policy. Status:Fully Paid\",\n",
    "                                                                \"Does not meet the credit policy. Status:Charged Off\",\n",
    "                                                                \"In Grace Period\", \"Late (16-30 days)\", \"Late (31-120 days)\",\n",
    "                                                                \"Default\", \"None\"])\n",
    "data_2015.pymnt_plan = pd.Categorical(data_2015.pymnt_plan.str.title(), categories=[\"Y\", \"N\", \"None\"])\n",
    "data_2015.purpose = pd.Categorical(data_2015.purpose.str.title(),\n",
    "                              categories=[\"Debt_Consolidation\", \"Credit_Card\", \"Home_Improvement\", \"Major_Purchase\", \n",
    "                                          \"Small_Business\", \"Car\", \"Wedding\", \"Medical\", \"Moving\", \"House\",\n",
    "                                          \"Educational\", \"Vacation\", \"Renewable_Energy\", \"Other\", \"None\"])\n",
    "data_2015.addr_state = pd.Categorical(data_2015.addr_state,\n",
    "                                 categories=[\"AK\", \"AL\", \"AR\", \"AZ\", \"CA\", \"CO\", \"CT\", \"DC\", \"DE\", \"FL\",\n",
    "                                             \"GA\", \"HI\", \"IA\", \"ID\", \"IL\", \"IN\", \"KS\", \"KY\", \"LA\", \"MA\",\n",
    "                                             \"MD\", \"ME\", \"MI\", \"MN\", \"MO\", \"MS\", \"MT\", \"NC\", \"ND\", \"NE\",\n",
    "                                             \"NH\", \"NJ\", \"NM\", \"NV\", \"NY\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\",\n",
    "                                             \"SC\", \"SD\", \"TN\", \"TX\", \"UT\", \"VA\", \"VT\", \"WA\", \"WI\", \"WV\",\n",
    "                                             \"WY\", \"None\"])\n",
    "data_2015.initial_list_status = pd.Categorical(data_2015.initial_list_status, categories=[\"f\", \"w\", \"None\"])\n",
    "data_2015.policy_code = pd.Categorical(data_2015.policy_code, categories=[\"1\", \"None\"])\n",
    "data_2015.application_type = pd.Categorical(data_2015.application_type.str.title(), categories=[\"Individual\", \"Joint\", \"None\"])\n",
    "data_2015.verification_status_joint = pd.Categorical(data_2015.verification_status_joint, categories=[\"Verified\", \"Source Verified\", \"Not Verified\", \"None\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate the categorical data conversion, we check a table listing Null values for each field. If any categories were excluded inadvertently, the *Null Count* in this table would show up as > 0. The *verification_status_joint* field does not appear to contain valid data for the datasets that have been analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame([[f, sum(pd.isnull(data[f]))] for f in data.columns[data.dtypes==\"category\"]],\n",
    "             columns=[\"Categorical Field\", \"Null Count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame([[f, sum(pd.isnull(data[f]))] for f in data.columns[data.dtypes==\"category\"]],\n",
    "             columns=[\"Categorical Field\", \"Null Count\"])\n",
    "\n",
    "pd.DataFrame([[f, sum(pd.isnull(data[f]))] for f in data.columns[data.dtypes==\"category\"]],\n",
    "             columns=[\"Categorical Field\", \"Null Count\"])\n",
    "\n",
    "pd.DataFrame([[f, sum(pd.isnull(data[f]))] for f in data.columns[data.dtypes==\"category\"]],\n",
    "             columns=[\"Categorical Field\", \"Null Count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some percentages are stored as strings (*int_rate*, *revol_util*). Here we convert them into a float by stripping the % symbol and dividing by 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def percent_to_float(s):\n",
    "    if (type(s) == str):\n",
    "        if (\"%\" in s):\n",
    "            return float(str(s).strip(\"%\"))/100\n",
    "        else:\n",
    "            if s == \"None\":\n",
    "                return np.nan\n",
    "            else:            \n",
    "                return s\n",
    "    else:\n",
    "        return s\n",
    "\n",
    "data.int_rate = [percent_to_float(s) for s in data.int_rate]\n",
    "data.revol_util = [percent_to_float(s) for s in data.revol_util]\n",
    "\n",
    "data_2007_2011.int_rate = [percent_to_float(s) for s in data_2007_2011.int_rate]\n",
    "data_2007_2011.revol_util = [percent_to_float(s) for s in data_2007_2011.revol_util]\n",
    "\n",
    "data_2014.int_rate = [percent_to_float(s) for s in data_2014.int_rate]\n",
    "data_2014.revol_util = [percent_to_float(s) for s in data_2014.revol_util]\n",
    "\n",
    "data_2015.int_rate = [percent_to_float(s) for s in data_2015.int_rate]\n",
    "data_2015.revol_util = [percent_to_float(s) for s in data_2015.revol_util]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final check of data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_2007_2011.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_2014.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_2015.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 5 remaining object fields are note easily parsed to a more convenient data structure. Those fields are listed in the table below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.select_dtypes(include=[\"object\"]).head()\n",
    "data_2007_2011.select_dtypes(include=[\"object\"]).head()\n",
    "data_2014.select_dtypes(include=[\"object\"]).head()\n",
    "data_2015.select_dtypes(include=[\"object\"]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print data.shape\n",
    "print data_2007_2011.shape\n",
    "fulldf = pd.concat([data, data_2007_2011, data_2014, data_2015])\n",
    "print fulldf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print fulldf[\"loan_status\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ok we dont want current or None status as we don't know what is going to happen with those.\n",
    "# next also late and grace period we are not sure about.\n",
    "# thirdly we remove loans that do not meet credit policy so we can convert our issue to a binar model\n",
    "fulldf = fulldf[((fulldf[\"loan_status\"] == \"Default\") | (fulldf[\"loan_status\"] == \"Charged Off\") | (fulldf[\"loan_status\"] == \"Fully Paid\"))]\n",
    "\n",
    "# Rename \"Charged off to default\"\n",
    "fulldf[\"loan_status\"] = fulldf[\"loan_status\"].replace(\"Charged Off\", \"Default\")\n",
    "print fulldf.shape\n",
    "print fulldf[\"loan_status\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y = fulldf[\"loan_status\"].values\n",
    "X = fulldf.drop(\"loan_status\", axis=1)\n",
    "cols = X.columns\n",
    "for col in cols:\n",
    "    if X[col].dtype != \"float64\":\n",
    "        X = X.drop(col, axis=1)\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!!(Which Predictors Are We Dropping Here, Why?)!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lending Club Data Set is unbalanced. The rate of positive examples is roughly 85%. As such, accuracy does not indicate the true performance of the model, because the accuracy will depend on the overall default rate of the test data set. For example, a model that predicts every example to be non-defaulting would still achieve 85% accuracy. Furthermore two of the models we tested, SVM and Logistic Regression, will bias toward classifications that occur more often. Therefore, we chose to look at sensitivity, defined as:\n",
    "\n",
    "<center>**sensitivity** $= TP / TP + FN$</center>\n",
    "\n",
    "where TP is the number of true positives, FN is the number of false negatives. \n",
    "\n",
    "Sensitivity represents the ratio of loans whose known value is positive (not default) to loans that our model predicted as positive (non-default). Keeping in mind that positive-skewed datasets inherently have a higher sensitivity, we measured and compared specificity as a control. Specificity is defined as:\n",
    "\n",
    "<center>**specificity** $= TN / TN + FP$</center>\n",
    "\n",
    "where TN is the number of true negatives and FP is the number of false positives. \n",
    "\n",
    "Sensitivity represents the ratio of loans whose known value is negative (default) to loans that our model predicted as negative (default).\n",
    "\n",
    "In an effort to combine sensitivity and specificity into a single metric, we chose to use G-mean as or chief evaliuation standard:\n",
    "\n",
    "G = $p / sensitivity ∗ specificity$\n",
    "\n",
    "To round off our performance metrics, we also looked at accuracy and precision:\n",
    "\n",
    "<center>**accuracy** $= TN + TP / N$</center>\n",
    "\n",
    "<center>**precision** $= TP / TP + FP$</center>\n",
    "\n",
    "To evaluate the performance of our models, we trained each model with the first 70% of the loans and test the trained models on the last evaluate the 30% of the loans in our dataset. \n",
    "\n",
    "After selecting the best predicting model based on specificity/g-score performance to optimize the detection of high-risk loans, we will calculate return on investment (ROI) by:\n",
    "\n",
    "<center>ROI = Total payment received by investors / Total amount committed by investors − 1</center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u> Overview</u> </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are four baseline models for the Lending Club analysis. These are simple models that treat the problem as binary classification. The outcomes can either be \"default\" or \"not default\". These models take simplicitic views of the problem and serve as baselines for comparison and sanity check when more sofisiticated models are built in later parts of the project.\n",
    "\n",
    "The models are:\n",
    "1. predict all outcomes to be \"not default\";\n",
    "2. predict all outcomes of borrowers with lowest credit scores to always :default\" and others \"not default\";\n",
    "3. model outcomes with a single predictor \"grade\";\n",
    "4. model outcomes with a single predictor \"interest rate\".\n",
    "\n",
    "The data cleaning steps are from milestone 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build our baselines, we first check the class proportions of our target categories. Since we are looking at classifying 'default' vs 'not default' our filtered data set has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Unique loan_status value:', np.unique(fulldf.loan_status)\n",
    "print 'Default/Paid ratio:', float(len(fulldf[fulldf.loan_status == 'Default'])) / float(len(fulldf[fulldf.loan_status == 'Fully Paid']))\n",
    "print 'Unique credit grades', np.unique(fulldf.grade)\n",
    "print 'Interest Rate Range', np.min(fulldf.int_rate), np.max(fulldf.int_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class proportion is roughly in 1 to 5 ratio so the imblace is not that bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Confusion Matrix - Function To Calculate CNF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "def cpt_cnf(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    cnf_matrix = confusion_matrix(y, y_pred)\n",
    "    print cnf_matrix\n",
    "    np.set_printoptions(precision=1)\n",
    "\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    img1 = plot_confusion_matrix(ax1, cnf_matrix, classes=[0, 1], title='Confusion matrix, without normalization')\n",
    "\n",
    "\n",
    "    # Plot normalized confusion matrix\n",
    "    img2 = plot_confusion_matrix(ax2, cnf_matrix, classes=[0, 1], normalize=True, title='Normalized confusion matrix')\n",
    "    plt.colorbar(img1, ax=ax1)\n",
    "    plt.colorbar(img2, ax=ax2)\n",
    "    plt.show()\n",
    "\n",
    "#This function prints and plots the confusion matrix.\n",
    "#Normalization can be applied by setting `normalize=True`.\n",
    "def plot_confusion_matrix(ax, cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "\n",
    "    img = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.set_title(title)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    ax.set_xticks(tick_marks)\n",
    "    ax.set_xticklabels(classes, rotation=45)\n",
    "    # ax.set_yticks([0.5, 2.5], classes)\n",
    "    ax.set_yticklabels([\"\", \"0\", \"\", \"1\"], rotation=45)\n",
    "\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        ax.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    ax.set_ylabel('True label')\n",
    "    ax.set_xlabel('Predicted label')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u> Baseline 1: All loans do not default</u> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred, y_prob):\n",
    "    print 'Overall Accuracy:', accuracy_score(y_true, y_pred)\n",
    "    print 'F1 Score:', f1_score(y_true, y_pred)\n",
    "    #setting \"0\" as positive case to indicate we are looking for \"Default\" cases\n",
    "    print 'F1 Score (0 as \"positive\" since we are looking for \"Default\" cases):', f1_score(y_true, y_pred, pos_label=0)\n",
    "    #pos_labe='None' is required: see https://github.com/scikit-learn/scikit-learn/issues/3122\n",
    "    print 'Weighted average F1 Score:', f1_score(y_true, y_pred, pos_label=None, average='weighted')\n",
    "    print 'Micro average F1 Score:', f1_score(y_true, y_pred, pos_label=None, average='micro')\n",
    "    print 'Macro average F1 Score:', f1_score(y_true, y_pred, pos_label=None, average='macro')\n",
    "    for i in np.unique(y_true):\n",
    "        y_class_true = y_true[y_true == i]\n",
    "        y_class_pred = y_pred[y_true == i]\n",
    "        print 'Class {:.0f} Accuracy:'.format(i), accuracy_score(y_class_true, y_class_pred)\n",
    "    print 'ROC AUC Score:', roc_auc_score(y_true, y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = pd.get_dummies(fulldf.loan_status)\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create response: we only take the first column from the one-hot encoded loan_status\n",
    "#since we only need to know if a loan is fully paid or not\n",
    "y = y.values[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#baseline model 1:\n",
    "#predicting all loan outcomes as \"not default\"\n",
    "def predict_outcome_baseline1(X):\n",
    "    results = np.empty(shape=(X.shape[0]), dtype=float)\n",
    "    results[:] = 1\n",
    "    return results\n",
    "\n",
    "def predict_proba_outcome_baseline1(X):\n",
    "    #in this simple baseline model, we predict \"defaults\" to be 0 and \"not default\" to be 1\n",
    "    #the returned probability is the probability of being \"not default\"\n",
    "    #only 0s and 1s are returned, there are no in-between probabilities\n",
    "    #that means, we the model predicts \"not default\", the model is certain that it is the case\n",
    "    return predict_outcome_baseline1(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = predict_outcome_baseline1(np.zeros([y.shape[0],1]))\n",
    "y_prob = predict_proba_outcome_baseline1(np.zeros([y.shape[0],1]))\n",
    "evaluate(y, y_pred, y_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has high overall accuracy becuase most loans do not default. However, the class 0 (default) breakdown shows that this model's accuracy on default loans is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u> Baseline 2: Predicting loans with lowest credit scores to default</u> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Credit Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model, we set a threshold of credit grade E and below to be a bad credit score. All loans with these scores are predicted as \"default\" while all others are predicted as \"not default\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#baseline model 2:\n",
    "#predicting all loan outcomes as \"not default\"\n",
    "def predict_outcome_baseline2(X):\n",
    "    #We take the credit grades E, F and G\n",
    "    #as \"bad\" credit scores\n",
    "    #we do this by summing up the corresponding one-hot encoded columns\n",
    "    results = X[:, -4:-1].sum(axis=1).reshape(-1, 1).astype('int')\n",
    "    #we need to the swap 0s and 1s to match the response encoding\n",
    "    results = -1 * ( results - 1 )\n",
    "    return results\n",
    "\n",
    "def predict_proba_outcome_baseline2(X):\n",
    "    #in this simple baseline model, we predict \"defaults\" to be 0 and \"not default\" to be 1\n",
    "    #the returned probability is the probability of being \"not default\"\n",
    "    #only 0s and 1s are returned, there are no in-between probabilities\n",
    "    #that means, we the model predicts \"not default\", the model is certain that it is the case\n",
    "    return predict_outcome_baseline2(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = pd.get_dummies(fulldf.grade)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = predict_outcome_baseline2(X.values)\n",
    "y_prob = predict_proba_outcome_baseline2(X.values)\n",
    "evaluate(y, y_pred, y_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has better (compared to baseline 1) prediction accuracy for default (class 0) loans. However, the values are still very low.\n",
    "\n",
    "Changing the threshold should allow us to trade accuracies of prediction of the two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Credit Threshhold - Descision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be interesting to see what a decision tree would pick as a threshold for this simplistic model.\n",
    "\n",
    "To do this, we encode the grade predictor as ordinal values and let the decision tree to pick a split point. We limit the depth to 1 to similate our decision process of the baseline model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://github.com/JWarmenhoven/ISLR-python\n",
    "def print_tree(estimator, features, class_names=None, filled=True):\n",
    "    tree = estimator\n",
    "    names = features\n",
    "    color = filled\n",
    "    classn = class_names\n",
    "    \n",
    "    dot_data = StringIO.StringIO()\n",
    "    export_graphviz(estimator, out_file=dot_data, feature_names=features, proportion=True, class_names=classn, filled=filled)\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "    return(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Categories of \"grade\" column:', [x for x in fulldf.grade.cat.categories]\n",
    "print 'Number of rows with grade=None:', len(fulldf[fulldf.grade=='None'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Since we do not have \"None\" values, our grades provide a sensible ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#here we encode the categorical loan_status column as numberic values\n",
    "#as the categories are ordinal\n",
    "X = fulldf.grade.cat.codes\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = X.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#use scikit-learn decision tree and set class_weight to handle the imbalance\n",
    "tree = DecisionTreeClassifier(class_weight='balanced', max_depth=1)\n",
    "tree.fit(X, y)\n",
    "y_pred = tree.predict(X)\n",
    "#here we use scikit-learn's predict_proba for the input to ROC_AUC_SCORE\n",
    "y_prob = tree.predict_proba(X)\n",
    "evaluate(y, y_pred, y_prob[:,1])\n",
    "\n",
    "cpt_cnf(tree, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the \"balanced\" option, scikit-learn decision tree has set a threshold so that the prediction accuracies of the two classes to be more comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#graph = print_tree(tree, ['grade'])\n",
    "#Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The split chosen (1.5) corresponds to a boundary between grades B and C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u> Baseline 3: Logistic Regression on Interest Rates</u> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = fulldf.int_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Oversampling\n",
    "#remove flutype)\n",
    "X = np.reshape(X, (X.shape[0], 1))\n",
    "y = np.reshape(y, (y.shape[0], 1))\n",
    "\n",
    "print X.shape\n",
    "print y.shape\n",
    "\n",
    "\n",
    "X = np.concatenate((X, y), axis=1)\n",
    "print X.shape\n",
    "\n",
    "unq, unq_idx = np.unique(X[:, -1], return_inverse=True)\n",
    "unq_cnt = np.bincount(unq_idx)\n",
    "cnt = np.max(unq_cnt)\n",
    "out = np.empty((cnt*len(unq) - len(X),) + X.shape[1:], X.dtype)\n",
    "slices = np.concatenate(([0], np.cumsum(cnt - unq_cnt)))\n",
    "for j in xrange(len(unq)):\n",
    "    indices = np.random.choice(np.where(unq_idx==j)[0], cnt - unq_cnt[j])\n",
    "    out[slices[j]:slices[j+1]] = X[indices]\n",
    "out = np.vstack((X, out))\n",
    "\n",
    "\n",
    "data_oversamp = out\n",
    "print data_oversamp.shape\n",
    "\n",
    "x_oversamp = data_oversamp[: , :-1]\n",
    "y_oversamp = data_oversamp[: , -1]\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_oversamp, y_oversamp, test_size=0.2)\n",
    "print x_train\n",
    "#print x_train[:, -1].mean()\n",
    "print y_train.mean()\n",
    "\n",
    "#print sum(x_train_ovr[:, -1]-y_train_ovr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Logistic Regresstion\n",
    "LOG_ = LogisticRegression(C = 1000000) #C=1000000,penalty='l1'\n",
    "model_LOG = LOG_.fit(x_train, y_train)\n",
    "y_pred = LOG_.predict(x_test)\n",
    "y_prob = LOG_.predict_proba(x_test)\n",
    "\n",
    "evaluate(y_test, y_pred, y_prob[:, 1])\n",
    "cpt_cnf(model_LOG, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Overview/Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!!(Insert Model Comparisons - Pros, Cons, Hypothesis, Previous Research)!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**!!(INSERT EVALUATION CRITERIA WRITEUP)!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = fulldf[\"loan_status\"].values\n",
    "df_for_drop = fulldf\n",
    "X_full = df_for_drop.drop(\"loan_status\", axis=1)\n",
    "x = X_full.values\n",
    "print X_full.shape\n",
    "print y.shape\n",
    "print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in fulldf:\n",
    "    if fulldf[col].dtype != \"float64\" and\\\n",
    "            fulldf[col].dtype != \"datetime64[ns]\" and\\\n",
    "            fulldf[col].dtype != \"object\":\n",
    "    #if fulldf[col].dtype = \"category\":\n",
    "        print fulldf[col].dtype, col\n",
    "        print fulldf[col].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get DF of all Non-Numeric Predictors\n",
    "cols = X_full.columns\n",
    "cols_to_drop = []\n",
    "for col in cols:\n",
    "    if X_full[col].dtype != \"object\" and\\\n",
    "            col != \"grade\" and col !=\"sub_grade\" and\\\n",
    "            fulldf[col].dtype != \"float64\" and\\\n",
    "            fulldf[col].dtype != \"datetime64[ns]\":\n",
    "        cols_to_drop.append(col)\n",
    "print cols_to_drop\n",
    "x_alpha = X_full[X_full.columns[[c in cols_to_drop for c in X_full.columns]]]\n",
    "x_numeric = X_full[X_full.columns[[not c in cols_to_drop for c in X_full.columns]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print x_alpha.columns\n",
    "print x_numeric.columns\n",
    "\n",
    "print len(x_alpha.columns), len(x_numeric.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### !!!! WARNING !!!! ### LOOP CRASHES KERNAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pandas One Hot Encoding #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = x_alpha.columns      \n",
    "#Create DF With All Predictors Encoded\n",
    "x_expanded = x_numeric\n",
    "for col in cols:\n",
    "    print col\n",
    "    col_series = pd.get_dummies(x_alpha[col])\n",
    "    x_expanded = pd.concat([x_numeric, pd.get_dummies(x_alpha)], axis=1)\n",
    "\n",
    "print x_expanded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in x_expanded.columns:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SKLearn One Hot Encoding #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(x_alpha)  \n",
    "OneHotEncoder(categorical_features='all', handle_unknown='error', n_values='auto', sparse=False)\n",
    "enc.n_values_\n",
    "enc.feature_indices_\n",
    "#enc.transform().toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = x_alpha.columns      \n",
    "#Create DF With All Predictors Encoded\n",
    "x_expanded = x_numeric\n",
    "for col in cols:\n",
    "    col_series = pd.get_dummies(x_alpha[col])\n",
    "    x_expanded = pd.concat([x_expanded, col_series], axis=1)\n",
    "    \n",
    "print x_expanded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_expanded, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Standard Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u> Overview</u> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first modeled defaults using logistic regression to learn more about the data features and get the basic performance of our prediction. To first get boundaries of iterations needed for Newton as well as understand predictive contribution from each data features, we trial-trained with a logistic classification on all features.\n",
    "\n",
    "The training and test converges to an optimal solution within 5 iterations, and overall we reached a test accuracy of 92.9% and a test specificity of 75.8%.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C = [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300] %// you must choose your own set of values for the parameters that you want to test. You can either do it this way by explicitly typing out a list\n",
    "S = 0:0.1:1 %// or you can do it this way using the : operator\n",
    "fscores = zeros(numel(C), numel(S)); %// Pre-allocation\n",
    "for c = 1:numel(C)   \n",
    "    for s = 1:numel(S)\n",
    "        vals = crossval(@(XTRAIN, YTRAIN, XVAL, YVAL)(fun(x_train, y_train, x_train, y_train, C(c), S(c)),input(trIdx,:),target(trIdx));\n",
    "        fscores(c,s) = mean(vals);\n",
    "    end\n",
    "end\n",
    "\n",
    "%// Then establish the C and S that gave you the bet f-score. Don't forget that c and s are just indexes though!\n",
    "[cbest, sbest] = find(fscores == max(fscores(:)));\n",
    "C_final = C(cbest);\n",
    "S_final = S(sbest);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** !! INSERT FEATURE MODELS !!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u> Bias Variance Tradeoff</u> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluation the tradeoff between bias and variance in our logistic model, we ran tested different sample sizes and charted the impact on accuracy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>(a) Training vs. test errors</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** !! INSERT GRAPH !!**\n",
    "Y-Axis = MSE\n",
    "X-axis = Training Sample Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> (b) Training vs. test specificity ** </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** !! INSERT GRAPH !!**\n",
    "Y-Axis = Speceficity (# correct prediction / # actual defaults)\n",
    "X-axis = Training Sample Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test and training error converged **!!(with a sample size >= 5,000)!!** and we see that we may have **!!(a high bias problem as increasing sample size still resulted in a >5% test error. From the sensitivity chart, however, we see that sensitivity fluctuates)**!! with additional sample size, suggesting that the default prediction might potentially benefit from filtering on existing features even though test error has stabilized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4><u> Feature Selection</u> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In exploring the dataset, we ran a logistic model on each each available predictor. We found that for prediction on default rate (specificity), the credit score for the borrower is the most predicative of all features, followed by borrower population; while interest rate has a negative impact as the number was subject to sporadic adjustment from Lending Club, and fields like loan description or borrower’s lower fico range, where there are a lot of zero values, would worsen the default prediction. After we filtered out features that decreased our test specificity, such as last_fico_range_low, installment, open_acc, desc, int_rate, we managed to bump specificity from **!!(75.8% to 77.1%)!!** without hurting overall test accuracy.\n",
    "\n",
    "Data From: http://cs229.stanford.edu/proj2015/199_report.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Drop above columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Table 1: Performance of Logistic Model with feature selection **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Num Newton Accu Prec Sens Spec G-mean\n",
    "30 Iterations 92.8 96.6 95.1 77.1 85.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Native Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u> Overview</u> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!! Native Bayes (Gaussian, Bernoulli, and Multinomial) | Laplace Smoothing = 1 !!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used a Laplace smoothing factor of 1 and ran Naive Bayes using Gaussian, Bernoulli, and Multinomial probability distributions.\n",
    "\n",
    "For Bernoulli Naive Bayes, where we require boolean feature values, we binarized the features and values. Multinomial probability distributions take discrete feature values, so we rounded any decimal feature values to the closest integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Distribution Accu Prec Sens Spec G-mean\n",
    "Gaussian 91.1 96.6 92.7 80.4 86.3\n",
    "Bernoulli 88.5 91.0 96.2 36.9 59.6\n",
    "Multinomial 61.9 88.7 64.4 45.8 54.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Naive Bayes returns the most desirable performance on the test dataset. Bernoulli and Multinomial significantly underperformed Gaussian Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u> Bias Variance Tradeoff</u> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to determine whether we are seeing high bias or\n",
    "high variance, we compare the training error to the test error\n",
    "for each case of Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an effort to reduce the error rate for Gaussian Naive Bayes, an additional feature of median income by zip code and descriptions was included in the dataset. The test error for Gaussian Naive Bayes decreased from 8.94% to 8.64% (by 0.3%). However, specificity decreased from 80.4% to 80.1% with the external dataset included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Distribution Accu Prec Sens Spec G-mean\n",
    "Gaussian 91.3 96.9 93.0 80.1 86.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u> Overview</u> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the training data is likely not linearly separable, and not guaranteed to be separable even in higher-dimensional feature spaces, we will use L1 (LASSO) regularization (soft margin SVM). \n",
    "\n",
    "For training data points (x(i), y(i)), the model is the result of the optimization:\n",
    "\n",
    "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/d9e948f6526c060d6cd63bd27631e213113c0ecd\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -2.171ex; width:27.957ex; height:5.509ex;\" alt=\"\\min _{w\\in \\mathbb {R} ^{p}}{\\frac {1}{n}}\\|{\\hat {X}}w-{\\hat {Y}}\\|^{2}+\\lambda \\|w\\|_{1}\">\n",
    "\n",
    "First, in an effort to prevent features with larger numeric values (absolute value) from dominating features with smaller numeric values, we normalized the dataset by scaling the values of each feature to [-1, 1]. Furthermore, normalizion prevents numeric problems such as overflows in situations where kernel values involve the inner products of feature vectors, which is anticipated in this instance. \n",
    "\n",
    "Note: The same scaling factor was applied to both the training and test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_predictors)\n",
    "scaler.fit(test_predictors)\n",
    "scaled_data_train = scaler.transform(train_predictors)\n",
    "scaled_data_test = scaler.transform(test_predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C = [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300] %// you must choose your own set of values for the parameters that you want to test. You can either do it this way by explicitly typing out a list\n",
    "S = 0:0.1:1 %// or you can do it this way using the : operator\n",
    "fscores = zeros(numel(C), numel(S)); %// Pre-allocation\n",
    "for c = 1:numel(C)   \n",
    "    for s = 1:numel(S)\n",
    "        vals = crossval(@(XTRAIN, YTRAIN, XVAL, YVAL)(fun(XTRAIN, YTRAIN, XVAL, YVAL, C(c), S(c)),input(trIdx,:),target(trIdx));\n",
    "        fscores(c,s) = mean(vals);\n",
    "    end\n",
    "end\n",
    "\n",
    "%// Then establish the C and S that gave you the bet f-score. Don't forget that c and s are just indexes though!\n",
    "[cbest, sbest] = find(fscores == max(fscores(:)));\n",
    "C_final = C(cbest);\n",
    "S_final = S(sbest);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM has a degrading performance on highly imbalanced datasets [10], so we should experiment with balancing the datasets as best as possible to improve SVM prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u>Balancing The Dataset</u> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u>Opitiization</u> </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM model performance depends on three inputs: the kernel, the kernel parmeters, and the soft margin parameter C. We attempted to optimize each of these values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernal Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We investigate some commonly used kernels (linear, polynomial, Gaussian radial basis function, and sigmoid) and compare performance. We used LibSVM [8] with default settings (C-SVC, C = 1, γ = 1/# of features, d = 3), and trained the model with the first 70% of the loans and tested the models on the last 30% of the loans in our dataset.\n",
    "\n",
    "Linear: $K(x, z) = x^Tz$ <br>\n",
    "Polynomial: $K(x, z) = (γ(x^Tz + 1))^d$ <br>\n",
    "RBF: $K(x, z) = e^(−γ||x−z||^2)$ <br>\n",
    "Sigmoid: $K(x, z) = tanh(γx^Tz + d)$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!!(SVM Results #1)!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Kernel Accu Prec Sens Spec G-mean\n",
    "Linear 93.0 96.6 95.3 77.7 86.1\n",
    "Polynomial 93.0 94.5 97.6 61.8 77.7\n",
    "RBF 92.8 93.9 98.0 57.5 75.1\n",
    "Sigmoid 90.8 91.8 98.2 41.0 63.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!!(Baseline Model #1)!!**, which merely predicts every loan to be non-defaulting, achieves an accuracy of 87.0%, which is the fraction of test data that are actually positive, so we see that SVM improves predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u> Bias Viariance Tradeoff</u> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "** !! INSERT FEATURE MODELS !!**\n",
    "Y-Axis = MSE\n",
    "X-axis = Training Sample Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run SVM with a linear kernel with variable number of training examples, then compare the training error with the test error to determine whether we are likely to be encountering high bias or high variance in our SVM model with the dataset that we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test and training errors converge quickly relative to the number of training examples available, and the gap between them is small, suggesting a high bias in the model. Thus we will increase the number of features by expanding zip code into census data, as well as identifying important words in the loan title and description. Adding median income, mean income, and population fields extrapolated from the zip code, we see a minute 0.1% increase in precision and 0.2% increase in specificity, with all other performance metrics remaining the same. Adding the words selected via TF-IDF as boolean features, we see an increase across all performance metrics, including a 0.6% boost in G-mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u>Soft Margin Parameters</u> </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We experimented with different values of the soft margin parameter C. We ran linear kernel SVM with $C ={10^−6, 10^−4, 10^−2, 1, 10^2}$. Ultimately, we found that using **!!($C = 1$)!!** yielded the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u> Overview</u> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!!(Train On Precision))!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = ensemble.RandomForestClassifier(n_jobs=-1,n_estimators=50,oob_score=True,max_depth=6)\n",
    "# set the various grid params to consider for tuning\n",
    "param_grid = {\n",
    "   'n_estimators': [500,700,1000],\n",
    "   'max_features': ['auto', 'sqrt', 'log2'],\n",
    "   'oob_score' :[True,False],\n",
    "   'max_depth' :[5,10,20,50,100,200],\n",
    "   'min_samples_leaf' : [1,10,20,50,100]\n",
    "}\n",
    "rf_cv = grid_search.GridSearchCV(n_jobs=2, estimator=rf, param_grid=param_grid, cv= 5)\n",
    "rf_cv.fit(train_predictors, train_labels)\n",
    "# Calculate the Best Random Forest Score\n",
    "rf = ensemble.RandomForestClassifier(n_jobs=-1,\n",
    "                                    max_features=rf_cv.best_params_['max_features'],\n",
    "                                    n_estimators=rf_cv.best_params_['n_estimators'],\n",
    "                                    oob_score=rf_cv.best_params_['oob_score'],\n",
    "                                    max_depth=rf_cv.best_params_['max_depth'],\n",
    "                                    min_samples_leaf=rf_cv.best_params_['min_samples_leaf']\n",
    "                                   )\n",
    "rf.fit(train_predictors, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%# read some training data\n",
    "[labels,data] = libsvmread('./heart_scale');\n",
    "\n",
    "%# grid of parameters\n",
    "folds = 5;\n",
    "[C,gamma] = meshgrid(-5:2:15, -15:2:3);\n",
    "\n",
    "%# grid search, and cross-validation\n",
    "cv_acc = zeros(numel(C),1);\n",
    "    for i=1:numel(C)\n",
    "cv_acc(i) = svmtrain(labels, data, ...\n",
    "                sprintf('-c %f -g %f -v %d', 2^C(i), 2^gamma(i), folds));\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C = [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300] %// you must choose your own set of values for the parameters that you want to test. You can either do it this way by explicitly typing out a list\n",
    "S = 0:0.1:1 %// or you can do it this way using the : operator\n",
    "fscores = zeros(numel(C), numel(S)); %// Pre-allocation\n",
    "for c = 1:numel(C)   \n",
    "    for s = 1:numel(S)\n",
    "        vals = crossval(@(XTRAIN, YTRAIN, XVAL, YVAL)(fun(XTRAIN, YTRAIN, XVAL, YVAL, C(c), S(c)),input(trIdx,:),target(trIdx));\n",
    "        fscores(c,s) = mean(vals);\n",
    "    end\n",
    "end\n",
    "\n",
    "%// Then establish the C and S that gave you the bet f-score. Don't forget that c and s are just indexes though!\n",
    "[cbest, sbest] = find(fscores == max(fscores(:)));\n",
    "C_final = C(cbest);\n",
    "S_final = S(sbest);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI. Cost Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u> Overview</u> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u> Defining Cost</u> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cost(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    cost_true_positive = 90\n",
    "    cost_true_negative = -10\n",
    "    cost_false_positive = -15010\n",
    "    cost_false_negative = -10\n",
    "        \n",
    "    n_true_positive = sum([(pred_val == 1) & (true_val == 1) for true_val, pred_val in zip(y_true, y_pred)])\n",
    "    n_true_negative = sum([(pred_val == 0) & (true_val == 0) for true_val, pred_val in zip(y_true, y_pred)])\n",
    "    n_false_positive = sum([(pred_val == 0) & (true_val == 1) for true_val, pred_val in zip(y_true, y_pred)])\n",
    "    n_false_negative = sum([(pred_val == 1) & (true_val == 0) for true_val, pred_val in zip(y_true, y_pred)])\n",
    "            \n",
    "    total_cost = (np.array([[cost_true_negative, cost_false_negative], [cost_false_positive, cost_true_positive]]) *\n",
    "                  np.array([[n_true_negative, n_false_negative], [n_false_positive, n_true_positive]])).sum()\n",
    "    \n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "def cv_cost(model, n_folds, X, y):\n",
    "    scores = pd.DataFrame({\"class\":[\"all\"] + list(np.unique(y)), \"total\":0, \"correct\":0, \"score\":0, \"cost\":0})\n",
    "    scores.set_index(\"class\", inplace=True)\n",
    "    kf = KFold(len(X), n_folds=n_folds, shuffle=True, random_state=0)\n",
    "    for train_index, test_index in kf:\n",
    "        train_X, test_X = X[train_index, :], X[test_index, :]\n",
    "        train_y, test_y = y.ravel()[train_index], y.ravel()[test_index]  \n",
    "        \n",
    "        model.fit(train_X, train_y)\n",
    "        \n",
    "        for cls in np.unique(y):\n",
    "            pred_y = model.predict(test_X[test_y==cls])\n",
    "\n",
    "            n_correct = sum(pred_y == cls)\n",
    "            n_total = sum(test_y==cls)\n",
    "        \n",
    "            cost_amt = cost(pred_y, [cls] * len(pred_y))\n",
    "            scores.loc[cls, \"correct\"] += n_correct\n",
    "            scores.loc[cls, \"total\"] += n_total\n",
    "            scores.loc[cls, \"cost\"] += cost_amt\n",
    "            scores.loc[\"all\", \"correct\"] += n_correct\n",
    "            scores.loc[\"all\", \"total\"] += n_total\n",
    "            scores.loc[\"all\", \"cost\"] += cost_amt\n",
    "\n",
    "    scores.score = scores.correct / scores.total\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u> Evaluation</u> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Baseline models\n",
    "# All positive (label every loan as safe)\n",
    "class Pos_model(object):\n",
    "    def fit(self, X, y):\n",
    "        return\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.array([1] * len(x))\n",
    "    \n",
    "    def score(self, x, y):\n",
    "        y_pred = self.predict(x)\n",
    "        y_err = y - y_pred\n",
    "        return len(y_err[y_err == 0]) * 1. / len(y_err)\n",
    "\n",
    "pos_model = Pos_model()\n",
    "cv_cost(pos_model, 5, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# All negative (label every loan as risky)\n",
    "class Neg_model(object):\n",
    "    def fit(self, X, y):\n",
    "        return\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.array([0] * len(x))\n",
    "    \n",
    "    def score(self, x, y):\n",
    "        y_pred = self.predict(x)\n",
    "        y_err = y - y_pred\n",
    "        return len(y_err[y_err == 0]) * 1. / len(y_err)\n",
    "\n",
    "neg_model = Neg_model()\n",
    "cv_cost(neg_model, 5, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Random (randomly predict safe or risky)\n",
    "class Random_model(object):\n",
    "    def fit(self, X, y):\n",
    "        return\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.random.randint(0, 2, len(x))\n",
    "    \n",
    "    def score(self, x, y):\n",
    "        y_pred = self.predict(x)\n",
    "        y_err = y - y_pred\n",
    "        return len(y_err[y_err == 0]) * 1. / len(y_err)\n",
    "\n",
    "random_model = Random_model()\n",
    "cv_cost(random_model, 5, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import quandl\n",
    "\n",
    "#state_list = [\"AK\", \"AL\", \"AR\", \"AZ\", \"CA\", \"CO\", \"CT\", \"DC\", \"DE\", \"FL\",\n",
    "#                                             \"GA\", \"HI\", \"IA\", \"ID\", \"IL\", \"IN\", \"KS\", \"KY\", \"LA\", \"MA\",\n",
    "#                                             \"MD\", \"ME\", \"MI\", \"MN\", \"MO\", \"MS\", \"MT\", \"NC\", \"ND\", \"NE\",\n",
    "#                                             \"NH\", \"NJ\", \"NM\", \"NV\", \"NY\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\",\n",
    "#                                             \"SC\", \"SD\", \"TN\", \"TX\", \"UT\", \"VA\", \"VT\", \"WA\", \"WI\", \"WV\",\n",
    "#                                             \"WY\"]\n",
    "#state_unemployment_data = {}\n",
    "#for i in state_list:\n",
    "#    try:\n",
    "#        print(i)\n",
    "#        result = quandl.get('FRED/{:s}UR'.format(i))\n",
    "#        state_unemployment_data[i] = result\n",
    "#    except RuntimeError as e:\n",
    "#        print(e)\n",
    "\n",
    "#check all indices are sorted by date\n",
    "#for s in state_unemployment_data.keys():\n",
    "#    if not state_unemployment_data[s].index.is_monotonic:\n",
    "#        print s\n",
    "\n",
    "#convert percentages to a value in the range [0, 1]\n",
    "#for s in state_unemployment_data.keys():\n",
    "#    state_unemployment_data[s]['VALUE'] = state_unemployment_data[s]['VALUE'] / 100.0\n",
    "\n",
    "#import pickle\n",
    "#with open('unemployment_rates_by_state.pkl', 'wb') as f:\n",
    "#    pickle.dump(state_unemployment_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('unemployment_rates_by_state.pkl', 'rb') as f:\n",
    "    state_unemployment_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state_unemployment_data['AK'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def unemp_rate(row):\n",
    "    try:\n",
    "        state = row.addr_state\n",
    "        date = pd.to_datetime(row.last_pymnt_d.strftime('%Y-%m'), format='%Y-%m')\n",
    "        result = state_unemployment_data[state][date:].iloc[0]\n",
    "    except:\n",
    "        result = np.NaN\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#add 1 extra column ('state_unemp_rate') to the data frames\n",
    "\n",
    "#this is taking a long time to run (10 mins on my computer - Ben)\n",
    "\n",
    "print 'data'\n",
    "data['state_unemp_rate']=data.apply(unemp_rate, axis=1)\n",
    "print 'data_2007_2011'\n",
    "data_2007_2011['state_unemp_rate']=data_2007_2011.apply(unemp_rate, axis=1)\n",
    "print 'data_2014'\n",
    "data_2014['state_unemp_rate']=data_2014.apply(unemp_rate, axis=1)\n",
    "print 'data_2015'\n",
    "data_2015['state_unemp_rate']=data_2015.apply(unemp_rate, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data_2007_2011 = data_2007_2011[data_2007_2011.columns[data.columns!='state_unemp_rate']]\n",
    "#data_2014 = data_2014[data_2014.columns[data.columns!='state_unemp_rate']]\n",
    "#data_2015 = data_2015[data_2015.columns[data.columns!='state_unemp_rate']]\n",
    "#data = data[data.columns[data.columns!='state_unemp_rate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VII. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After comparing each model, we found that **!!(Naive Bayes with Gaussian)!!** performs the best with default prediction **!!(80.1% sensitivity)!!**. We speculate that **!!(Naive Bayes with Gaussian)!!** is slightly better than the other models for 2 potential reasons:\n",
    "\n",
    "- Naive Bayes model works well with independent feature sets [1], and the training features that we selected are possibly either independent or have evenly distributed dependencies.\n",
    "- Some of the key features that we used, like credit scores and regional population, might be distributed in Gaussian, which would allow the Gaussian-assumption model to perform better.\n",
    "\n",
    "Predicting default rates in isolation is intersting and can be valuable to individual, small time investors. However, the real commercial value of better performing default prediction models is **!!(Insert Conclusion)!!**.  As such, we calculated ROI as **!!(mentioned in Method section)!!**. Currently, the overall return from Lending Club (including interest and late fee earned and principal recovered) by all investors divided by their initial principal is **!!(10.1% [?])!!**. If we apply the model prediction to avoid investing on the predicted-to-default note and calculate the return based on only predicted-positive loans, we can increase the return on investment of the test set from **!!(10.1% to over 15.7% (50% growth))!!**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIII. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the comparison of multiple models, including Logistic Regression, SVM, Naive Bayes, Random Forest, and **!!(new models)!!** and different fine-tuning mechanisms, we found that **!!(Naive Bayes with Guassian)!!** performs the best at predicting default rate (optimizing specificity). And by applying our best-performing model, we saw that the investment return from Lending Club can potentially grow by 50%. Some future work that could further improve the prediction includes:\n",
    "- We see high-bias problems across all models, so the predictions could benefit from more features, such as stock market or housing trend, so as to include more macroeconomic factors into the models.\n",
    "- Not all default cases are the same, and some late payments could still be recovered later on, so instead of a binary classification, multinomial predictions could be employed to take into account the different types of default so as to make a more granular prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1]  Zhang, Harry. \"The optimality of naive Bayes.\" AA 1.2 (2004): 3. <br>\n",
    "[2] He, He, and Ali Ghodsi. \"Rare class classification by support vector machine.\" Pattern Recognition (ICPR), 2010 20th International Conference on. IEEE, 2010. <br>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:cs109a_proj]",
   "language": "python",
   "name": "conda-env-cs109a_proj-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
